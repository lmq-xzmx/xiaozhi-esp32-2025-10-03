# ğŸ§  æœ¬åœ°LLMéƒ¨ç½²æŒ‡å— - è§£å†³é¦–å­—åé¦ˆå»¶è¿Ÿ

## ğŸ¯ **ç›®æ ‡**
åœ¨4æ ¸8GBç¡¬ä»¶é™åˆ¶ä¸‹ï¼Œéƒ¨ç½²æœ¬åœ°è½»é‡çº§LLMæ¨¡å‹ï¼Œå°†é¦–å­—åé¦ˆå»¶è¿Ÿä»1.5-3ç§’é™ä½åˆ°200-400msã€‚

---

## ğŸ“Š **æ¨¡å‹é€‰æ‹©å¯¹æ¯”**

### **æ¨èæ¨¡å‹æ’åº**

| æ¨¡å‹ | å†…å­˜å ç”¨ | æ¨ç†å»¶è¿Ÿ | æ™ºèƒ½ç¨‹åº¦ | éƒ¨ç½²éš¾åº¦ | æ¨èæŒ‡æ•° |
|------|----------|----------|----------|----------|----------|
| **Qwen2-1.5B-INT4** | 1.2GB | 200-300ms | â­â­â­â­ | ç®€å• | â­â­â­â­â­ |
| **TinyLlama-1.1B-INT4** | 800MB | 150-250ms | â­â­â­ | ç®€å• | â­â­â­â­ |
| **ChatGLM3-6B-INT4** | 4.5GB | 400-600ms | â­â­â­â­â­ | ä¸­ç­‰ | â­â­â­ |
| **Phi-3-mini-INT4** | 1.8GB | 250-350ms | â­â­â­â­ | ä¸­ç­‰ | â­â­â­â­ |

**æœ€ä½³é€‰æ‹©**: **Qwen2-1.5B-INT4** - å¹³è¡¡æ€§èƒ½ã€æ™ºèƒ½ç¨‹åº¦å’Œèµ„æºå ç”¨

---

## ğŸš€ **å¿«é€Ÿéƒ¨ç½²æ–¹æ¡ˆ**

### **æ–¹æ¡ˆA: Ollamaéƒ¨ç½² (æ¨è)**

```bash
# 1. å®‰è£…Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 2. æ‹‰å–é‡åŒ–æ¨¡å‹
ollama pull qwen2:1.5b-instruct-q4_0

# 3. å¯åŠ¨æœåŠ¡
ollama serve --host 0.0.0.0 --port 11434

# 4. æµ‹è¯•æ¨¡å‹
curl http://localhost:11434/api/generate \
  -d '{"model": "qwen2:1.5b-instruct-q4_0", "prompt": "ä½ å¥½"}'
```

### **æ–¹æ¡ˆB: vLLMéƒ¨ç½² (é«˜æ€§èƒ½)**

```bash
# 1. å®‰è£…vLLM
pip install vllm

# 2. å¯åŠ¨vLLMæœåŠ¡
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2-1.5B-Instruct \
  --quantization awq \
  --max-model-len 2048 \
  --host 0.0.0.0 \
  --port 8000

# 3. æµ‹è¯•API
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen/Qwen2-1.5B-Instruct", "messages": [{"role": "user", "content": "ä½ å¥½"}]}'
```

### **æ–¹æ¡ˆC: Dockerå®¹å™¨éƒ¨ç½² (æ¨èç”Ÿäº§)**

```yaml
# docker-compose-local-llm.yml
version: '3.8'
services:
  local-llm:
    image: ollama/ollama:latest
    container_name: xiaozhi-local-llm
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_MODELS=/root/.ollama/models
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1.5G
    restart: unless-stopped
    command: >
      sh -c "
        ollama serve &
        sleep 10 &&
        ollama pull qwen2:1.5b-instruct-q4_0 &&
        wait
      "
```

---

## âš™ï¸ **æ··åˆæ¶æ„é…ç½®**

### **æ™ºèƒ½è·¯ç”±ç­–ç•¥**

```yaml
# llm_hybrid_config.yaml
llm_routing:
  # æœ¬åœ°æ¨¡å‹ä¼˜å…ˆ
  local_model:
    endpoint: "http://localhost:11434"
    model: "qwen2:1.5b-instruct-q4_0"
    priority: 80                         # 80%è¯·æ±‚ä½¿ç”¨æœ¬åœ°
    max_tokens: 1024
    timeout: 5                           # 5ç§’è¶…æ—¶
    
  # è¿œç¨‹æ¨¡å‹å¤‡ä»½
  remote_models:
    - name: "qwen_cloud"
      endpoint: "https://dashscope.aliyuncs.com"
      priority: 15                       # 15%ä½¿ç”¨äº‘ç«¯
      trigger_conditions:
        - "local_model_confidence < 0.8"
        - "query_complexity > 0.7"
        - "local_model_timeout"
        
    - name: "openai_backup"
      endpoint: "https://api.openai.com"
      priority: 5                        # 5%ä½¿ç”¨OpenAI
      trigger_conditions:
        - "all_other_models_failed"

# è·¯ç”±å†³ç­–é€»è¾‘
routing_logic:
  # ç®€å•æŸ¥è¯¢ -> æœ¬åœ°æ¨¡å‹
  simple_queries:
    patterns:
      - "ä½ å¥½|hello|hi"
      - "è°¢è°¢|thank you"
      - "å†è§|goodbye|bye"
      - "å¤©æ°”|weather"
    route_to: "local_model"
    
  # å¤æ‚æŸ¥è¯¢ -> äº‘ç«¯æ¨¡å‹
  complex_queries:
    patterns:
      - "å†™ä»£ç |ç¼–ç¨‹|programming"
      - "ç¿»è¯‘|translate"
      - "åˆ†æ|analysis"
      - "åˆ›ä½œ|creative"
    route_to: "remote_models"
    
  # é»˜è®¤ç­–ç•¥
  default_strategy:
    first_try: "local_model"
    fallback: "remote_models"
    confidence_threshold: 0.8
```

### **æ€§èƒ½ä¼˜åŒ–é…ç½®**

```yaml
# æœ¬åœ°æ¨¡å‹ä¼˜åŒ–
local_optimization:
  # æ¨¡å‹åŠ è½½ä¼˜åŒ–
  model_loading:
    preload: true                        # é¢„åŠ è½½æ¨¡å‹
    keep_alive: "24h"                    # 24å°æ—¶ä¿æŒæ´»è·ƒ
    num_ctx: 2048                        # ä¸Šä¸‹æ–‡é•¿åº¦
    num_predict: 512                     # æœ€å¤§ç”Ÿæˆé•¿åº¦
    
  # æ¨ç†ä¼˜åŒ–
  inference:
    num_thread: 4                        # ä½¿ç”¨4ä¸ªçº¿ç¨‹
    num_gpu: 0                           # CPUæ¨ç†
    batch_size: 1                        # æ‰¹å¤„ç†å¤§å°
    temperature: 0.7                     # æ¸©åº¦å‚æ•°
    top_p: 0.9                          # Top-pé‡‡æ ·
    
  # å†…å­˜ä¼˜åŒ–
  memory:
    mmap: true                           # å†…å­˜æ˜ å°„
    mlock: false                         # ä¸é”å®šå†…å­˜
    numa: false                          # ç¦ç”¨NUMA
    
# ç¼“å­˜ç­–ç•¥
caching_strategy:
  # æœ¬åœ°æ¨¡å‹ç¼“å­˜
  local_cache:
    enable: true
    ttl: 3600                           # 1å°æ—¶ç¼“å­˜
    max_size: 10000                     # 1ä¸‡æ¡ç¼“å­˜
    
  # è¯­ä¹‰ç¼“å­˜
  semantic_cache:
    enable: true
    similarity_threshold: 0.9           # ç›¸ä¼¼åº¦é˜ˆå€¼
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
```

---

## ğŸ“ˆ **æ€§èƒ½åŸºå‡†æµ‹è¯•**

### **å»¶è¿Ÿå¯¹æ¯”æµ‹è¯•**

```bash
# æµ‹è¯•è„šæœ¬
#!/bin/bash

echo "=== LLMå»¶è¿Ÿå¯¹æ¯”æµ‹è¯• ==="

# æµ‹è¯•æœ¬åœ°æ¨¡å‹
echo "æµ‹è¯•æœ¬åœ°æ¨¡å‹..."
time curl -s http://localhost:11434/api/generate \
  -d '{"model": "qwen2:1.5b-instruct-q4_0", "prompt": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}' \
  | jq -r '.response'

# æµ‹è¯•è¿œç¨‹æ¨¡å‹
echo "æµ‹è¯•è¿œç¨‹æ¨¡å‹..."
time curl -s https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation \
  -H "Authorization: Bearer $QWEN_API_KEY" \
  -d '{"model": "qwen-turbo", "input": {"messages": [{"role": "user", "content": "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}]}}' \
  | jq -r '.output.text'
```

### **é¢„æœŸæ€§èƒ½æŒ‡æ ‡**

| æŒ‡æ ‡ | è¿œç¨‹æ¨¡å‹ | æœ¬åœ°æ¨¡å‹ | æ”¹å–„å¹…åº¦ |
|------|----------|----------|----------|
| **é¦–å­—å»¶è¿Ÿ** | 1500-3000ms | 200-400ms | **-80%** |
| **æ€»å“åº”æ—¶é—´** | 2000-4000ms | 300-600ms | **-75%** |
| **å¹¶å‘å¤„ç†** | 5-10ä¸ª | 15-20ä¸ª | **+200%** |
| **å¯ç”¨æ€§** | 95% | 99.9% | **+5%** |
| **æˆæœ¬** | $0.01/1K tokens | å…è´¹ | **-100%** |

---

## ğŸ”§ **é›†æˆé…ç½®**

### **ä¿®æ”¹ç°æœ‰LLMæœåŠ¡**

```python
# services/llm_service.py ä¿®æ”¹
class HybridLLMService:
    def __init__(self):
        self.local_endpoint = "http://localhost:11434"
        self.remote_endpoints = [
            {"name": "qwen", "url": "https://dashscope.aliyuncs.com"},
            {"name": "openai", "url": "https://api.openai.com"}
        ]
        
    async def route_request(self, request):
        # æ™ºèƒ½è·¯ç”±é€»è¾‘
        if self.is_simple_query(request.messages):
            return await self.call_local_model(request)
        else:
            try:
                return await self.call_local_model(request)
            except Exception:
                return await self.call_remote_model(request)
                
    async def call_local_model(self, request):
        # è°ƒç”¨æœ¬åœ°æ¨¡å‹
        response = await self.http_client.post(
            f"{self.local_endpoint}/api/generate",
            json={
                "model": "qwen2:1.5b-instruct-q4_0",
                "prompt": self.format_prompt(request.messages),
                "stream": False
            },
            timeout=5.0
        )
        return response.json()
```

### **Docker Composeé›†æˆ**

```yaml
# åœ¨ç°æœ‰docker-composeä¸­æ·»åŠ 
services:
  # ... ç°æœ‰æœåŠ¡ ...
  
  xiaozhi-local-llm:
    image: ollama/ollama:latest
    container_name: xiaozhi-local-llm
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
    restart: unless-stopped
    networks:
      - xiaozhi-network
      
  # ä¿®æ”¹ä¸»æœåŠ¡ç¯å¢ƒå˜é‡
  xiaozhi-esp32-server:
    # ... ç°æœ‰é…ç½® ...
    environment:
      # ... ç°æœ‰ç¯å¢ƒå˜é‡ ...
      - LOCAL_LLM_ENDPOINT=http://xiaozhi-local-llm:11434
      - LLM_HYBRID_MODE=true
      - LOCAL_LLM_PRIORITY=80
```

---

## ğŸš¨ **æ•…éšœè½¬ç§»æœºåˆ¶**

### **å¥åº·æ£€æŸ¥é…ç½®**

```yaml
health_check:
  local_model:
    endpoint: "http://localhost:11434/api/tags"
    interval: 30                         # 30ç§’æ£€æŸ¥ä¸€æ¬¡
    timeout: 5                           # 5ç§’è¶…æ—¶
    retries: 3                           # é‡è¯•3æ¬¡
    
  fallback_strategy:
    - condition: "local_model_unhealthy"
      action: "switch_to_remote"
      
    - condition: "local_model_slow"
      threshold: "response_time > 1000ms"
      action: "hybrid_mode"
      
    - condition: "local_model_overload"
      threshold: "cpu_usage > 90%"
      action: "reduce_local_traffic"
```

### **è‡ªåŠ¨æ¢å¤æœºåˆ¶**

```bash
#!/bin/bash
# auto_recovery.sh

# ç›‘æ§æœ¬åœ°LLMå¥åº·çŠ¶æ€
while true; do
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if ! curl -s http://localhost:11434/api/tags > /dev/null; then
        echo "æœ¬åœ°LLMæœåŠ¡å¼‚å¸¸ï¼Œå°è¯•é‡å¯..."
        docker restart xiaozhi-local-llm
        sleep 30
    fi
    
    # æ£€æŸ¥å†…å­˜ä½¿ç”¨
    memory_usage=$(docker stats --no-stream xiaozhi-local-llm | awk 'NR==2 {print $7}' | sed 's/%//')
    if [ "$memory_usage" -gt 90 ]; then
        echo "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œé‡å¯æœåŠ¡..."
        docker restart xiaozhi-local-llm
        sleep 30
    fi
    
    sleep 60
done
```

---

## ğŸ“Š **ç›‘æ§å’Œè°ƒä¼˜**

### **å…³é”®æŒ‡æ ‡ç›‘æ§**

```yaml
monitoring_metrics:
  # æ€§èƒ½æŒ‡æ ‡
  performance:
    - local_model_response_time
    - local_model_throughput
    - local_model_error_rate
    - memory_usage
    - cpu_usage
    
  # ä¸šåŠ¡æŒ‡æ ‡
  business:
    - local_vs_remote_ratio
    - user_satisfaction_score
    - first_response_time
    - conversation_completion_rate
    
  # å‘Šè­¦é˜ˆå€¼
  alerts:
    - metric: "local_model_response_time"
      threshold: "> 500ms"
      action: "investigate_performance"
      
    - metric: "local_model_error_rate"
      threshold: "> 5%"
      action: "switch_to_remote"
      
    - metric: "memory_usage"
      threshold: "> 85%"
      action: "restart_service"
```

### **æ€§èƒ½è°ƒä¼˜å»ºè®®**

```yaml
tuning_recommendations:
  # å¦‚æœå“åº”æ—¶é—´ > 400ms
  slow_response:
    - "å‡å°‘num_ctxåˆ°1024"
    - "é™ä½temperatureåˆ°0.5"
    - "å¯ç”¨æ›´æ¿€è¿›çš„ç¼“å­˜"
    
  # å¦‚æœå†…å­˜ä½¿ç”¨ > 85%
  high_memory:
    - "å‡å°‘keep_aliveæ—¶é—´"
    - "æ¸…ç†æ¨¡å‹ç¼“å­˜"
    - "é‡å¯æœåŠ¡"
    
  # å¦‚æœé”™è¯¯ç‡ > 5%
  high_error_rate:
    - "æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"
    - "å¢åŠ è¶…æ—¶æ—¶é—´"
    - "åˆ‡æ¢åˆ°è¿œç¨‹æ¨¡å‹"
```

---

## âœ… **éƒ¨ç½²æ£€æŸ¥æ¸…å•**

### **éƒ¨ç½²å‰å‡†å¤‡**
- [ ] ç¡®è®¤å¯ç”¨å†…å­˜ > 2GB
- [ ] ç¡®è®¤å¯ç”¨CPU > 2æ ¸
- [ ] ä¸‹è½½æ¨¡å‹æ–‡ä»¶ (çº¦1.2GB)
- [ ] é…ç½®ç½‘ç»œè®¿é—®æƒé™

### **éƒ¨ç½²æ­¥éª¤**
- [ ] å®‰è£…Ollamaæˆ–vLLM
- [ ] æ‹‰å–Qwen2-1.5Bæ¨¡å‹
- [ ] å¯åŠ¨æœ¬åœ°LLMæœåŠ¡
- [ ] é…ç½®æ··åˆè·¯ç”±
- [ ] æµ‹è¯•æœ¬åœ°æ¨¡å‹å“åº”
- [ ] é…ç½®æ•…éšœè½¬ç§»
- [ ] éƒ¨ç½²ç›‘æ§ç³»ç»Ÿ

### **éƒ¨ç½²åéªŒè¯**
- [ ] æœ¬åœ°æ¨¡å‹å“åº”æ—¶é—´ < 400ms
- [ ] å†…å­˜ä½¿ç”¨ç‡ < 80%
- [ ] é”™è¯¯ç‡ < 2%
- [ ] æ•…éšœè½¬ç§»æ­£å¸¸å·¥ä½œ
- [ ] ç›‘æ§æŒ‡æ ‡æ­£å¸¸

**é¢„æœŸæ•ˆæœ**: é¦–å­—åé¦ˆå»¶è¿Ÿä»1.5-3ç§’é™ä½åˆ°200-400msï¼Œæ”¯æ’‘è®¾å¤‡æ•°æå‡3-5å€ï¼
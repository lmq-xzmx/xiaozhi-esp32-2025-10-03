#!/usr/bin/env python3
"""
LLMæµå¼ä¼˜åŒ–å™¨ - ç¡®ä¿æµå¼é…ç½®æ­£ç¡®å¯ç”¨
é’ˆå¯¹182.44.78.40:8002 APIçš„æµå¼ä¼˜åŒ–
"""

import os
import json
import asyncio
from typing import Dict, Any

class LLMStreamingOptimizer:
    """LLMæµå¼ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.config = {
            # ğŸš€ æµå¼æ ¸å¿ƒé…ç½®
            "LLM_STREAM_ENABLED": True,
            "LLM_STREAM_FIRST_TOKEN": True,     # é¦–å­—æµå¼
            "LLM_DEFAULT_STREAM": True,         # é»˜è®¤å¯ç”¨æµå¼
            
            # âš¡ è¿æ¥ä¼˜åŒ–
            "LLM_CONNECTION_POOL_SIZE": 50,     # è¿æ¥æ± å¤§å°
            "LLM_MAX_RETRIES": 3,               # é‡è¯•æ¬¡æ•°
            "LLM_TIMEOUT": 30,                  # è¶…æ—¶æ—¶é—´
            "LLM_KEEPALIVE": True,              # ä¿æŒè¿æ¥
            
            # ğŸ¯ APIä¼˜åŒ– (é’ˆå¯¹182.44.78.40:8002)
            "LLM_API_BASE": "http://182.44.78.40:8002",
            "LLM_STREAM_BUFFER_SIZE": 8192,     # æµå¼ç¼“å†²åŒº
            "LLM_CHUNK_SIZE": 1024,             # æ•°æ®å—å¤§å°
            
            # ğŸ“¡ ç½‘ç»œä¼˜åŒ–
            "LLM_TCP_NODELAY": True,            # ç¦ç”¨Nagle
            "LLM_HTTP_VERSION": "1.1",          # HTTPç‰ˆæœ¬
            "LLM_COMPRESSION": True,            # å¯ç”¨å‹ç¼©
            
            # ğŸ”§ æ€§èƒ½ä¼˜åŒ–
            "LLM_ASYNC_WORKERS": 10,            # å¼‚æ­¥å·¥ä½œçº¿ç¨‹
            "LLM_CACHE_ENABLED": True,          # å¯ç”¨ç¼“å­˜
            "LLM_CACHE_TTL": 3600,              # ç¼“å­˜TTL
        }
    
    def apply_optimization(self) -> Dict[str, Any]:
        """åº”ç”¨LLMæµå¼ä¼˜åŒ–"""
        print("ğŸš€ LLMæµå¼ä¼˜åŒ– - é¦–å­—å»¶è¿Ÿæ€æ‰‹")
        print("=" * 60)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        for key, value in self.config.items():
            os.environ[key] = str(value)
            print(f"âœ… {key} = {value}")
        
        print("\nğŸ¯ ä¼˜åŒ–ç›®æ ‡:")
        print("   é¦–å­—å»¶è¿Ÿ: å‡å°‘30-50ms")
        print("   æµå¼ç¨³å®šæ€§: +95%")
        print("   è¿æ¥å¤ç”¨: +80%")
        
        return self.config
    
    def patch_llm_service(self):
        """ä¿®è¡¥LLMæœåŠ¡ï¼Œç¡®ä¿æµå¼é»˜è®¤å¯ç”¨"""
        llm_service_path = "/root/xiaozhi-server/services/llm_service.py"
        
        try:
            with open(llm_service_path, "r", encoding="utf-8") as f:
                content = f.read()
            
            # ä¿®æ”¹é»˜è®¤streamå‚æ•°
            if "stream: bool = False" in content:
                content = content.replace(
                    "stream: bool = False",
                    "stream: bool = True  # é»˜è®¤å¯ç”¨æµå¼"
                )
                
                with open(llm_service_path, "w", encoding="utf-8") as f:
                    f.write(content)
                
                print("âœ… LLMæœåŠ¡å·²ä¿®è¡¥ - é»˜è®¤å¯ç”¨æµå¼")
            else:
                print("â„¹ï¸  LLMæœåŠ¡å·²æ˜¯æµå¼æ¨¡å¼")
                
        except Exception as e:
            print(f"âš ï¸  ä¿®è¡¥LLMæœåŠ¡å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    optimizer = LLMStreamingOptimizer()
    
    # åº”ç”¨ä¼˜åŒ–
    config = optimizer.apply_optimization()
    
    # ä¿®è¡¥LLMæœåŠ¡
    optimizer.patch_llm_service()
    
    print("\nğŸš€ LLMæµå¼ä¼˜åŒ–å®Œæˆ!")
    print("ğŸ“ å»ºè®®é‡å¯LLMæœåŠ¡ä»¥åº”ç”¨é…ç½®")
    
    return config

if __name__ == "__main__":
    main()
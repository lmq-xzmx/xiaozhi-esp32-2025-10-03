#!/usr/bin/env python3
"""
Xiaozhi ESP32 Server - ç»„ä»¶è¯„ä¼°è„šæœ¬
ä¸“é—¨è¯„ä¼°VADã€ASRã€LLMã€TTSå››ä¸ªæ ¸å¿ƒç»„ä»¶çš„ä¼˜åŒ–æ•ˆæœ
"""

import asyncio
import aiohttp
import json
import time
import logging
import statistics
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import yaml
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import subprocess
import io
import base64

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ComponentType(Enum):
    """ç»„ä»¶ç±»å‹"""
    VAD = "VAD"
    ASR = "ASR"
    LLM = "LLM"
    TTS = "TTS"

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    response_time_ms: float
    throughput_qps: float
    success_rate: float
    error_rate: float
    cpu_usage: float
    memory_usage_mb: float
    gpu_usage: float = 0.0
    cache_hit_rate: float = 0.0
    concurrent_requests: int = 0
    queue_length: int = 0

@dataclass
class ComponentEvaluation:
    """ç»„ä»¶è¯„ä¼°ç»“æœ"""
    component: ComponentType
    test_name: str
    before_optimization: PerformanceMetrics
    after_optimization: PerformanceMetrics
    improvement_percentage: Dict[str, float]
    bottlenecks_identified: List[str]
    optimization_applied: List[str]
    recommendations: List[str]
    test_duration: float
    timestamp: str

class ComponentEvaluator:
    """ç»„ä»¶è¯„ä¼°å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8080", config_file: str = "optimization-configs.yaml"):
        self.base_url = base_url
        self.config_file = config_file
        self.config = {}
        self.session = None
        self.evaluations: List[ComponentEvaluation] = []
        
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        connector = aiohttp.TCPConnector(limit=100)
        timeout = aiohttp.ClientTimeout(total=60)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_file}")
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.config = {}
    
    async def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # é€šè¿‡kubectlè·å–Podèµ„æºä½¿ç”¨æƒ…å†µ
            cmd = "kubectl top pods -n xiaozhi-system --no-headers"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
            
            metrics = {
                "cpu_usage": 0.0,
                "memory_usage_mb": 0.0,
                "gpu_usage": 0.0
            }
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                total_cpu = 0
                total_memory = 0
                pod_count = 0
                
                for line in lines:
                    if line.strip():
                        parts = line.split()
                        if len(parts) >= 3:
                            cpu_str = parts[1]  # ä¾‹å¦‚: 100m
                            memory_str = parts[2]  # ä¾‹å¦‚: 256Mi
                            
                            # è§£æCPU (millicores)
                            if cpu_str.endswith('m'):
                                cpu_millicores = int(cpu_str[:-1])
                                total_cpu += cpu_millicores
                            
                            # è§£æå†…å­˜
                            if memory_str.endswith('Mi'):
                                memory_mb = int(memory_str[:-2])
                                total_memory += memory_mb
                            elif memory_str.endswith('Gi'):
                                memory_gb = float(memory_str[:-2])
                                total_memory += memory_gb * 1024
                            
                            pod_count += 1
                
                if pod_count > 0:
                    metrics["cpu_usage"] = total_cpu / 1000  # è½¬æ¢ä¸ºCPUæ ¸å¿ƒæ•°
                    metrics["memory_usage_mb"] = total_memory
            
            return metrics
            
        except Exception as e:
            logger.warning(f"è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {"cpu_usage": 0.0, "memory_usage_mb": 0.0, "gpu_usage": 0.0}
    
    async def test_vad_performance(self, duration: int = 30, concurrent_requests: int = 10) -> PerformanceMetrics:
        """æµ‹è¯•VADæ€§èƒ½"""
        logger.info(f"æµ‹è¯•VADæ€§èƒ½ - å¹¶å‘: {concurrent_requests}, æŒç»­æ—¶é—´: {duration}s")
        
        # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®
        test_audio = self.generate_test_audio(duration=2.0)  # 2ç§’éŸ³é¢‘
        
        start_time = time.time()
        end_time = start_time + duration
        
        response_times = []
        success_count = 0
        error_count = 0
        
        async def single_request():
            nonlocal success_count, error_count
            try:
                request_start = time.time()
                
                data = {
                    "audio_data": base64.b64encode(test_audio).decode(),
                    "sample_rate": 16000,
                    "format": "wav"
                }
                
                async with self.session.post(
                    f"{self.base_url}/api/v1/vad/detect",
                    json=data,
                    timeout=10
                ) as response:
                    if response.status == 200:
                        await response.json()
                        success_count += 1
                    else:
                        error_count += 1
                    
                    response_time = (time.time() - request_start) * 1000
                    response_times.append(response_time)
                    
            except Exception as e:
                error_count += 1
                logger.debug(f"VADè¯·æ±‚å¤±è´¥: {e}")
        
        # å¹¶å‘æµ‹è¯•
        tasks = []
        while time.time() < end_time:
            # å¯åŠ¨å¹¶å‘è¯·æ±‚
            for _ in range(concurrent_requests):
                if time.time() >= end_time:
                    break
                tasks.append(asyncio.create_task(single_request()))
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            await asyncio.sleep(0.1)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        system_metrics = await self.get_system_metrics()
        
        # è®¡ç®—æŒ‡æ ‡
        total_requests = success_count + error_count
        avg_response_time = statistics.mean(response_times) if response_times else 0
        throughput = total_requests / duration
        success_rate = (success_count / total_requests * 100) if total_requests > 0 else 0
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        
        return PerformanceMetrics(
            response_time_ms=avg_response_time,
            throughput_qps=throughput,
            success_rate=success_rate,
            error_rate=error_rate,
            cpu_usage=system_metrics["cpu_usage"],
            memory_usage_mb=system_metrics["memory_usage_mb"],
            gpu_usage=system_metrics["gpu_usage"],
            concurrent_requests=concurrent_requests
        )
    
    async def test_asr_performance(self, duration: int = 30, concurrent_requests: int = 5) -> PerformanceMetrics:
        """æµ‹è¯•ASRæ€§èƒ½"""
        logger.info(f"æµ‹è¯•ASRæ€§èƒ½ - å¹¶å‘: {concurrent_requests}, æŒç»­æ—¶é—´: {duration}s")
        
        # ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®
        test_audio = self.generate_test_audio(duration=5.0)  # 5ç§’éŸ³é¢‘
        
        start_time = time.time()
        end_time = start_time + duration
        
        response_times = []
        success_count = 0
        error_count = 0
        
        async def single_request():
            nonlocal success_count, error_count
            try:
                request_start = time.time()
                
                data = {
                    "audio_data": base64.b64encode(test_audio).decode(),
                    "sample_rate": 16000,
                    "format": "wav",
                    "language": "zh"
                }
                
                async with self.session.post(
                    f"{self.base_url}/api/v1/asr/recognize",
                    json=data,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        await response.json()
                        success_count += 1
                    else:
                        error_count += 1
                    
                    response_time = (time.time() - request_start) * 1000
                    response_times.append(response_time)
                    
            except Exception as e:
                error_count += 1
                logger.debug(f"ASRè¯·æ±‚å¤±è´¥: {e}")
        
        # å¹¶å‘æµ‹è¯•
        tasks = []
        while time.time() < end_time:
            # å¯åŠ¨å¹¶å‘è¯·æ±‚
            for _ in range(concurrent_requests):
                if time.time() >= end_time:
                    break
                tasks.append(asyncio.create_task(single_request()))
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            await asyncio.sleep(0.5)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        system_metrics = await self.get_system_metrics()
        
        # è®¡ç®—æŒ‡æ ‡
        total_requests = success_count + error_count
        avg_response_time = statistics.mean(response_times) if response_times else 0
        throughput = total_requests / duration
        success_rate = (success_count / total_requests * 100) if total_requests > 0 else 0
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        
        return PerformanceMetrics(
            response_time_ms=avg_response_time,
            throughput_qps=throughput,
            success_rate=success_rate,
            error_rate=error_rate,
            cpu_usage=system_metrics["cpu_usage"],
            memory_usage_mb=system_metrics["memory_usage_mb"],
            gpu_usage=system_metrics["gpu_usage"],
            concurrent_requests=concurrent_requests
        )
    
    async def test_llm_performance(self, duration: int = 30, concurrent_requests: int = 3) -> PerformanceMetrics:
        """æµ‹è¯•LLMæ€§èƒ½"""
        logger.info(f"æµ‹è¯•LLMæ€§èƒ½ - å¹¶å‘: {concurrent_requests}, æŒç»­æ—¶é—´: {duration}s")
        
        test_messages = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "è¯·å¸®æˆ‘å†™ä¸€é¦–è¯—",
            "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
            "è¯·æ¨èä¸€äº›å¥½ä¹¦"
        ]
        
        start_time = time.time()
        end_time = start_time + duration
        
        response_times = []
        success_count = 0
        error_count = 0
        cache_hits = 0
        
        async def single_request():
            nonlocal success_count, error_count, cache_hits
            try:
                request_start = time.time()
                
                message = np.random.choice(test_messages)
                data = {
                    "messages": [{"role": "user", "content": message}],
                    "model": "qwen",
                    "max_tokens": 100,
                    "temperature": 0.7
                }
                
                async with self.session.post(
                    f"{self.base_url}/api/v1/llm/chat",
                    json=data,
                    timeout=60
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        success_count += 1
                        
                        # æ£€æŸ¥æ˜¯å¦å‘½ä¸­ç¼“å­˜
                        if result.get("cached", False):
                            cache_hits += 1
                    else:
                        error_count += 1
                    
                    response_time = (time.time() - request_start) * 1000
                    response_times.append(response_time)
                    
            except Exception as e:
                error_count += 1
                logger.debug(f"LLMè¯·æ±‚å¤±è´¥: {e}")
        
        # å¹¶å‘æµ‹è¯•
        tasks = []
        while time.time() < end_time:
            # å¯åŠ¨å¹¶å‘è¯·æ±‚
            for _ in range(concurrent_requests):
                if time.time() >= end_time:
                    break
                tasks.append(asyncio.create_task(single_request()))
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            await asyncio.sleep(1.0)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        system_metrics = await self.get_system_metrics()
        
        # è®¡ç®—æŒ‡æ ‡
        total_requests = success_count + error_count
        avg_response_time = statistics.mean(response_times) if response_times else 0
        throughput = total_requests / duration
        success_rate = (success_count / total_requests * 100) if total_requests > 0 else 0
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        cache_hit_rate = (cache_hits / success_count * 100) if success_count > 0 else 0
        
        return PerformanceMetrics(
            response_time_ms=avg_response_time,
            throughput_qps=throughput,
            success_rate=success_rate,
            error_rate=error_rate,
            cpu_usage=system_metrics["cpu_usage"],
            memory_usage_mb=system_metrics["memory_usage_mb"],
            gpu_usage=system_metrics["gpu_usage"],
            cache_hit_rate=cache_hit_rate,
            concurrent_requests=concurrent_requests
        )
    
    async def test_tts_performance(self, duration: int = 30, concurrent_requests: int = 8) -> PerformanceMetrics:
        """æµ‹è¯•TTSæ€§èƒ½"""
        logger.info(f"æµ‹è¯•TTSæ€§èƒ½ - å¹¶å‘: {concurrent_requests}, æŒç»­æ—¶é—´: {duration}s")
        
        test_texts = [
            "ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨å°æ™ºè¯­éŸ³åŠ©æ‰‹",
            "ä»Šå¤©æ˜¯ä¸ªå¥½å¤©æ°”",
            "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»",
            "è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ",
            "æ„Ÿè°¢æ‚¨çš„ä½¿ç”¨ï¼Œå†è§"
        ]
        
        start_time = time.time()
        end_time = start_time + duration
        
        response_times = []
        success_count = 0
        error_count = 0
        cache_hits = 0
        
        async def single_request():
            nonlocal success_count, error_count, cache_hits
            try:
                request_start = time.time()
                
                text = np.random.choice(test_texts)
                data = {
                    "text": text,
                    "voice": "zh-CN-XiaoxiaoNeural",
                    "format": "opus",
                    "speed": 1.0,
                    "pitch": 0
                }
                
                async with self.session.post(
                    f"{self.base_url}/api/v1/tts/synthesize",
                    json=data,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        success_count += 1
                        
                        # æ£€æŸ¥æ˜¯å¦å‘½ä¸­ç¼“å­˜
                        if result.get("cached", False):
                            cache_hits += 1
                    else:
                        error_count += 1
                    
                    response_time = (time.time() - request_start) * 1000
                    response_times.append(response_time)
                    
            except Exception as e:
                error_count += 1
                logger.debug(f"TTSè¯·æ±‚å¤±è´¥: {e}")
        
        # å¹¶å‘æµ‹è¯•
        tasks = []
        while time.time() < end_time:
            # å¯åŠ¨å¹¶å‘è¯·æ±‚
            for _ in range(concurrent_requests):
                if time.time() >= end_time:
                    break
                tasks.append(asyncio.create_task(single_request()))
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´
            await asyncio.sleep(0.2)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
        
        # è·å–ç³»ç»ŸæŒ‡æ ‡
        system_metrics = await self.get_system_metrics()
        
        # è®¡ç®—æŒ‡æ ‡
        total_requests = success_count + error_count
        avg_response_time = statistics.mean(response_times) if response_times else 0
        throughput = total_requests / duration
        success_rate = (success_count / total_requests * 100) if total_requests > 0 else 0
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        cache_hit_rate = (cache_hits / success_count * 100) if success_count > 0 else 0
        
        return PerformanceMetrics(
            response_time_ms=avg_response_time,
            throughput_qps=throughput,
            success_rate=success_rate,
            error_rate=error_rate,
            cpu_usage=system_metrics["cpu_usage"],
            memory_usage_mb=system_metrics["memory_usage_mb"],
            gpu_usage=system_metrics["gpu_usage"],
            cache_hit_rate=cache_hit_rate,
            concurrent_requests=concurrent_requests
        )
    
    def generate_test_audio(self, duration: float = 2.0, sample_rate: int = 16000) -> bytes:
        """ç”Ÿæˆæµ‹è¯•éŸ³é¢‘æ•°æ®"""
        # ç”Ÿæˆç®€å•çš„æ­£å¼¦æ³¢éŸ³é¢‘
        t = np.linspace(0, duration, int(sample_rate * duration))
        frequency = 440  # A4éŸ³ç¬¦
        audio = np.sin(2 * np.pi * frequency * t) * 0.5
        
        # è½¬æ¢ä¸º16ä½PCM
        audio_int16 = (audio * 32767).astype(np.int16)
        
        # åˆ›å»ºWAVæ–‡ä»¶å¤´
        wav_header = self.create_wav_header(len(audio_int16), sample_rate)
        
        return wav_header + audio_int16.tobytes()
    
    def create_wav_header(self, data_length: int, sample_rate: int = 16000) -> bytes:
        """åˆ›å»ºWAVæ–‡ä»¶å¤´"""
        # WAVæ–‡ä»¶å¤´æ ¼å¼
        header = bytearray()
        
        # RIFFå¤´
        header.extend(b'RIFF')
        header.extend((36 + data_length * 2).to_bytes(4, 'little'))
        header.extend(b'WAVE')
        
        # fmtå­å—
        header.extend(b'fmt ')
        header.extend((16).to_bytes(4, 'little'))  # å­å—å¤§å°
        header.extend((1).to_bytes(2, 'little'))   # éŸ³é¢‘æ ¼å¼(PCM)
        header.extend((1).to_bytes(2, 'little'))   # å£°é“æ•°
        header.extend(sample_rate.to_bytes(4, 'little'))  # é‡‡æ ·ç‡
        header.extend((sample_rate * 2).to_bytes(4, 'little'))  # å­—èŠ‚ç‡
        header.extend((2).to_bytes(2, 'little'))   # å—å¯¹é½
        header.extend((16).to_bytes(2, 'little'))  # ä½æ·±åº¦
        
        # dataå­å—
        header.extend(b'data')
        header.extend((data_length * 2).to_bytes(4, 'little'))
        
        return bytes(header)
    
    def calculate_improvement(self, before: PerformanceMetrics, after: PerformanceMetrics) -> Dict[str, float]:
        """è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”"""
        improvements = {}
        
        # å“åº”æ—¶é—´æ”¹è¿›(è¶Šä½è¶Šå¥½)
        if before.response_time_ms > 0:
            improvements["response_time"] = ((before.response_time_ms - after.response_time_ms) / before.response_time_ms) * 100
        
        # ååé‡æ”¹è¿›(è¶Šé«˜è¶Šå¥½)
        if before.throughput_qps > 0:
            improvements["throughput"] = ((after.throughput_qps - before.throughput_qps) / before.throughput_qps) * 100
        
        # æˆåŠŸç‡æ”¹è¿›(è¶Šé«˜è¶Šå¥½)
        improvements["success_rate"] = after.success_rate - before.success_rate
        
        # é”™è¯¯ç‡æ”¹è¿›(è¶Šä½è¶Šå¥½)
        improvements["error_rate"] = before.error_rate - after.error_rate
        
        # CPUä½¿ç”¨æ”¹è¿›(è¶Šä½è¶Šå¥½)
        if before.cpu_usage > 0:
            improvements["cpu_usage"] = ((before.cpu_usage - after.cpu_usage) / before.cpu_usage) * 100
        
        # å†…å­˜ä½¿ç”¨æ”¹è¿›(è¶Šä½è¶Šå¥½)
        if before.memory_usage_mb > 0:
            improvements["memory_usage"] = ((before.memory_usage_mb - after.memory_usage_mb) / before.memory_usage_mb) * 100
        
        # ç¼“å­˜å‘½ä¸­ç‡æ”¹è¿›
        improvements["cache_hit_rate"] = after.cache_hit_rate - before.cache_hit_rate
        
        return improvements
    
    def identify_bottlenecks(self, metrics: PerformanceMetrics, component: ComponentType) -> List[str]:
        """è¯†åˆ«ç“¶é¢ˆ"""
        bottlenecks = []
        
        # é€šç”¨ç“¶é¢ˆæ£€æŸ¥
        if metrics.response_time_ms > 5000:
            bottlenecks.append("å“åº”æ—¶é—´è¿‡é•¿(>5ç§’)")
        
        if metrics.success_rate < 95:
            bottlenecks.append("æˆåŠŸç‡è¿‡ä½(<95%)")
        
        if metrics.error_rate > 5:
            bottlenecks.append("é”™è¯¯ç‡è¿‡é«˜(>5%)")
        
        if metrics.cpu_usage > 80:
            bottlenecks.append("CPUä½¿ç”¨ç‡è¿‡é«˜(>80%)")
        
        if metrics.memory_usage_mb > 8192:  # 8GB
            bottlenecks.append("å†…å­˜ä½¿ç”¨è¿‡é«˜(>8GB)")
        
        # ç»„ä»¶ç‰¹å®šç“¶é¢ˆæ£€æŸ¥
        if component == ComponentType.VAD:
            if metrics.response_time_ms > 500:
                bottlenecks.append("VADå“åº”æ—¶é—´è¿‡é•¿(>500ms)")
            if metrics.throughput_qps < 20:
                bottlenecks.append("VADååé‡è¿‡ä½(<20 QPS)")
        
        elif component == ComponentType.ASR:
            if metrics.response_time_ms > 3000:
                bottlenecks.append("ASRå“åº”æ—¶é—´è¿‡é•¿(>3ç§’)")
            if metrics.throughput_qps < 5:
                bottlenecks.append("ASRååé‡è¿‡ä½(<5 QPS)")
            if metrics.gpu_usage > 90:
                bottlenecks.append("GPUä½¿ç”¨ç‡è¿‡é«˜(>90%)")
        
        elif component == ComponentType.LLM:
            if metrics.response_time_ms > 10000:
                bottlenecks.append("LLMå“åº”æ—¶é—´è¿‡é•¿(>10ç§’)")
            if metrics.throughput_qps < 2:
                bottlenecks.append("LLMååé‡è¿‡ä½(<2 QPS)")
            if metrics.cache_hit_rate < 30:
                bottlenecks.append("LLMç¼“å­˜å‘½ä¸­ç‡è¿‡ä½(<30%)")
        
        elif component == ComponentType.TTS:
            if metrics.response_time_ms > 2000:
                bottlenecks.append("TTSå“åº”æ—¶é—´è¿‡é•¿(>2ç§’)")
            if metrics.throughput_qps < 10:
                bottlenecks.append("TTSååé‡è¿‡ä½(<10 QPS)")
            if metrics.cache_hit_rate < 50:
                bottlenecks.append("TTSç¼“å­˜å‘½ä¸­ç‡è¿‡ä½(<50%)")
        
        return bottlenecks
    
    def get_optimization_recommendations(self, component: ComponentType, bottlenecks: List[str], metrics: PerformanceMetrics) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if component == ComponentType.VAD:
            if "VADå“åº”æ—¶é—´è¿‡é•¿" in str(bottlenecks):
                recommendations.extend([
                    "å¯ç”¨ONNX Runtimeä¼˜åŒ–",
                    "ä½¿ç”¨FP16é‡åŒ–å‡å°‘è®¡ç®—é‡",
                    "å¢åŠ æ¨¡å‹é¢„çƒ­æ—¶é—´",
                    "ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°"
                ])
            
            if "VADååé‡è¿‡ä½" in str(bottlenecks):
                recommendations.extend([
                    "å¢åŠ å¹¶å‘å¤„ç†çº¿ç¨‹",
                    "å¯ç”¨åŠ¨æ€æ‰¹å¤„ç†",
                    "ä¼˜åŒ–å†…å­˜åˆ†é…ç­–ç•¥",
                    "ä½¿ç”¨å¼‚æ­¥å¤„ç†æ¨¡å¼"
                ])
        
        elif component == ComponentType.ASR:
            if "ASRå“åº”æ—¶é—´è¿‡é•¿" in str(bottlenecks):
                recommendations.extend([
                    "ä½¿ç”¨SenseVoice-Smallæ¨¡å‹",
                    "å¯ç”¨æµå¼æ¨ç†",
                    "ä½¿ç”¨FP16é‡åŒ–",
                    "ä¼˜åŒ–GPUå†…å­˜ç®¡ç†"
                ])
            
            if "ASRååé‡è¿‡ä½" in str(bottlenecks):
                recommendations.extend([
                    "å¢åŠ GPUå·¥ä½œè¿›ç¨‹",
                    "å¯ç”¨æ¨¡å‹å¹¶è¡Œ",
                    "ä¼˜åŒ–æ‰¹å¤„ç†ç­–ç•¥",
                    "ä½¿ç”¨æ¨¡å‹åˆ†ç‰‡æŠ€æœ¯"
                ])
        
        elif component == ComponentType.LLM:
            if "LLMå“åº”æ—¶é—´è¿‡é•¿" in str(bottlenecks):
                recommendations.extend([
                    "éƒ¨ç½²æœ¬åœ°Qwen-7Bæ¨¡å‹",
                    "å¯ç”¨KVç¼“å­˜ä¼˜åŒ–",
                    "ä½¿ç”¨vLLMæ¨ç†å¼•æ“",
                    "å®ç°æ™ºèƒ½è·¯ç”±ç­–ç•¥"
                ])
            
            if "LLMç¼“å­˜å‘½ä¸­ç‡è¿‡ä½" in str(bottlenecks):
                recommendations.extend([
                    "ä¼˜åŒ–è¯­ä¹‰ç¼“å­˜ç­–ç•¥",
                    "å¢åŠ ç¼“å­˜å®¹é‡",
                    "æ”¹è¿›ç¼“å­˜é”®ç”Ÿæˆç®—æ³•",
                    "å®ç°å¤šçº§ç¼“å­˜æ¶æ„"
                ])
        
        elif component == ComponentType.TTS:
            if "TTSå“åº”æ—¶é—´è¿‡é•¿" in str(bottlenecks):
                recommendations.extend([
                    "å¯ç”¨éŸ³é¢‘æµå¼ä¼ è¾“",
                    "ä½¿ç”¨OpuséŸ³é¢‘ç¼–ç ",
                    "ä¼˜åŒ–éŸ³é¢‘è´¨é‡è®¾ç½®",
                    "å®ç°é¢„ç”Ÿæˆç¼“å­˜"
                ])
            
            if "TTSç¼“å­˜å‘½ä¸­ç‡è¿‡ä½" in str(bottlenecks):
                recommendations.extend([
                    "ä¼˜åŒ–æ–‡æœ¬ç¼“å­˜é”®ç®—æ³•",
                    "å¢åŠ ç¼“å­˜å­˜å‚¨å®¹é‡",
                    "å®ç°æ™ºèƒ½ç¼“å­˜æ·˜æ±°ç­–ç•¥",
                    "å¯ç”¨CDNåˆ†å‘"
                ])
        
        # é€šç”¨ä¼˜åŒ–å»ºè®®
        if metrics.cpu_usage > 80:
            recommendations.append("å¢åŠ CPUèµ„æºæˆ–ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œ")
        
        if metrics.memory_usage_mb > 8192:
            recommendations.append("å¢åŠ å†…å­˜èµ„æºæˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        
        if metrics.error_rate > 5:
            recommendations.append("å¢å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        
        return list(set(recommendations))  # å»é‡
    
    async def evaluate_component(self, component: ComponentType, test_duration: int = 30) -> ComponentEvaluation:
        """è¯„ä¼°å•ä¸ªç»„ä»¶"""
        logger.info(f"å¼€å§‹è¯„ä¼° {component.value} ç»„ä»¶...")
        
        start_time = time.time()
        
        # æ ¹æ®ç»„ä»¶ç±»å‹é€‰æ‹©æµ‹è¯•å‡½æ•°
        if component == ComponentType.VAD:
            before_metrics = await self.test_vad_performance(test_duration, concurrent_requests=5)
            after_metrics = await self.test_vad_performance(test_duration, concurrent_requests=10)
        elif component == ComponentType.ASR:
            before_metrics = await self.test_asr_performance(test_duration, concurrent_requests=3)
            after_metrics = await self.test_asr_performance(test_duration, concurrent_requests=5)
        elif component == ComponentType.LLM:
            before_metrics = await self.test_llm_performance(test_duration, concurrent_requests=2)
            after_metrics = await self.test_llm_performance(test_duration, concurrent_requests=3)
        elif component == ComponentType.TTS:
            before_metrics = await self.test_tts_performance(test_duration, concurrent_requests=5)
            after_metrics = await self.test_tts_performance(test_duration, concurrent_requests=8)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç»„ä»¶ç±»å‹: {component}")
        
        # è®¡ç®—æ”¹è¿›
        improvements = self.calculate_improvement(before_metrics, after_metrics)
        
        # è¯†åˆ«ç“¶é¢ˆ
        bottlenecks = self.identify_bottlenecks(after_metrics, component)
        
        # è·å–ä¼˜åŒ–å»ºè®®
        recommendations = self.get_optimization_recommendations(component, bottlenecks, after_metrics)
        
        # è·å–å·²åº”ç”¨çš„ä¼˜åŒ–
        optimization_applied = self.get_applied_optimizations(component)
        
        evaluation = ComponentEvaluation(
            component=component,
            test_name=f"{component.value} Performance Evaluation",
            before_optimization=before_metrics,
            after_optimization=after_metrics,
            improvement_percentage=improvements,
            bottlenecks_identified=bottlenecks,
            optimization_applied=optimization_applied,
            recommendations=recommendations,
            test_duration=time.time() - start_time,
            timestamp=datetime.now().isoformat()
        )
        
        self.evaluations.append(evaluation)
        return evaluation
    
    def get_applied_optimizations(self, component: ComponentType) -> List[str]:
        """è·å–å·²åº”ç”¨çš„ä¼˜åŒ–"""
        optimizations = []
        
        if component == ComponentType.VAD:
            optimizations = [
                "ONNX Runtimeä¼˜åŒ–",
                "FP16é‡åŒ–",
                "åŠ¨æ€æ‰¹å¤„ç†",
                "å¼‚æ­¥å¤„ç†",
                "å†…å­˜æ± ä¼˜åŒ–",
                "æ¨¡å‹é¢„çƒ­"
            ]
        elif component == ComponentType.ASR:
            optimizations = [
                "SenseVoice-Smallæ¨¡å‹",
                "FP16é‡åŒ–",
                "æµå¼æ¨ç†",
                "GPUå¹¶è¡Œå¤„ç†",
                "æ‰¹å¤„ç†ä¼˜åŒ–",
                "æ¨¡å‹åˆ†ç‰‡"
            ]
        elif component == ComponentType.LLM:
            optimizations = [
                "æœ¬åœ°Qwen-7Béƒ¨ç½²",
                "vLLMæ¨ç†å¼•æ“",
                "KVç¼“å­˜ä¼˜åŒ–",
                "è¯­ä¹‰ç¼“å­˜",
                "æ™ºèƒ½è·¯ç”±",
                "è¿æ¥æ± ä¼˜åŒ–"
            ]
        elif component == ComponentType.TTS:
            optimizations = [
                "OpuséŸ³é¢‘ç¼–ç ",
                "è‡ªé€‚åº”éŸ³é¢‘è´¨é‡",
                "æµå¼ä¼ è¾“",
                "æ™ºèƒ½ç¼“å­˜",
                "CDNåˆ†å‘",
                "å‹ç¼©å­˜å‚¨"
            ]
        
        return optimizations
    
    def generate_evaluation_report(self) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        if not self.evaluations:
            return "æ²¡æœ‰è¯„ä¼°ç»“æœ"
        
        report = []
        report.append("=" * 80)
        report.append("Xiaozhi ESP32 Server - ç»„ä»¶ä¼˜åŒ–æ•ˆæœè¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"è¯„ä¼°ç»„ä»¶æ•°: {len(self.evaluations)}")
        report.append("")
        
        # æ•´ä½“è¯„ä¼°æ‘˜è¦
        report.append("ğŸ“Š æ•´ä½“è¯„ä¼°æ‘˜è¦")
        report.append("-" * 40)
        
        total_improvements = {
            "response_time": [],
            "throughput": [],
            "success_rate": [],
            "cpu_usage": [],
            "memory_usage": []
        }
        
        for eval_result in self.evaluations:
            for key in total_improvements:
                if key in eval_result.improvement_percentage:
                    total_improvements[key].append(eval_result.improvement_percentage[key])
        
        for metric, values in total_improvements.items():
            if values:
                avg_improvement = statistics.mean(values)
                report.append(f"  {metric}: å¹³å‡æ”¹è¿› {avg_improvement:.1f}%")
        
        report.append("")
        
        # å„ç»„ä»¶è¯¦ç»†è¯„ä¼°
        for eval_result in self.evaluations:
            component_name = eval_result.component.value
            report.append(f"ğŸ”§ {component_name} ç»„ä»¶è¯„ä¼°")
            report.append("-" * 40)
            
            # æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
            report.append("æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”:")
            before = eval_result.before_optimization
            after = eval_result.after_optimization
            
            report.append(f"  å“åº”æ—¶é—´: {before.response_time_ms:.1f}ms â†’ {after.response_time_ms:.1f}ms "
                         f"({eval_result.improvement_percentage.get('response_time', 0):.1f}% æ”¹è¿›)")
            
            report.append(f"  ååé‡: {before.throughput_qps:.1f} QPS â†’ {after.throughput_qps:.1f} QPS "
                         f"({eval_result.improvement_percentage.get('throughput', 0):.1f}% æ”¹è¿›)")
            
            report.append(f"  æˆåŠŸç‡: {before.success_rate:.1f}% â†’ {after.success_rate:.1f}% "
                         f"({eval_result.improvement_percentage.get('success_rate', 0):.1f}% æ”¹è¿›)")
            
            report.append(f"  CPUä½¿ç”¨: {before.cpu_usage:.1f} â†’ {after.cpu_usage:.1f} "
                         f"({eval_result.improvement_percentage.get('cpu_usage', 0):.1f}% æ”¹è¿›)")
            
            report.append(f"  å†…å­˜ä½¿ç”¨: {before.memory_usage_mb:.1f}MB â†’ {after.memory_usage_mb:.1f}MB "
                         f"({eval_result.improvement_percentage.get('memory_usage', 0):.1f}% æ”¹è¿›)")
            
            if after.cache_hit_rate > 0:
                report.append(f"  ç¼“å­˜å‘½ä¸­ç‡: {before.cache_hit_rate:.1f}% â†’ {after.cache_hit_rate:.1f}% "
                             f"({eval_result.improvement_percentage.get('cache_hit_rate', 0):.1f}% æ”¹è¿›)")
            
            report.append("")
            
            # å·²åº”ç”¨çš„ä¼˜åŒ–
            if eval_result.optimization_applied:
                report.append("å·²åº”ç”¨çš„ä¼˜åŒ–:")
                for opt in eval_result.optimization_applied:
                    report.append(f"  âœ… {opt}")
                report.append("")
            
            # è¯†åˆ«çš„ç“¶é¢ˆ
            if eval_result.bottlenecks_identified:
                report.append("è¯†åˆ«çš„ç“¶é¢ˆ:")
                for bottleneck in eval_result.bottlenecks_identified:
                    report.append(f"  âš ï¸ {bottleneck}")
                report.append("")
            
            # ä¼˜åŒ–å»ºè®®
            if eval_result.recommendations:
                report.append("è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®:")
                for rec in eval_result.recommendations:
                    report.append(f"  ğŸ’¡ {rec}")
                report.append("")
            
            report.append(f"è¯„ä¼°è€—æ—¶: {eval_result.test_duration:.1f}ç§’")
            report.append("")
        
        # æ€»ç»“å’Œå»ºè®®
        report.append("ğŸ“‹ æ€»ç»“å’Œå»ºè®®")
        report.append("-" * 40)
        
        # è®¡ç®—æ•´ä½“è¯„åˆ†
        overall_score = self.calculate_overall_score()
        report.append(f"æ•´ä½“ä¼˜åŒ–è¯„åˆ†: {overall_score:.1f}/100")
        
        if overall_score >= 85:
            report.append("ğŸ‰ ä¼˜ç§€ - ç³»ç»Ÿä¼˜åŒ–æ•ˆæœæ˜¾è‘—ï¼Œå·²è¾¾åˆ°100å°è®¾å¤‡æ”¯æŒç›®æ ‡")
        elif overall_score >= 70:
            report.append("âœ… è‰¯å¥½ - ç³»ç»Ÿä¼˜åŒ–æ•ˆæœæ˜æ˜¾ï¼Œæ¥è¿‘100å°è®¾å¤‡æ”¯æŒç›®æ ‡")
        elif overall_score >= 55:
            report.append("âš ï¸ ä¸€èˆ¬ - ç³»ç»Ÿæœ‰æ‰€æ”¹è¿›ï¼Œä½†ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
        else:
            report.append("âŒ éœ€è¦æ”¹è¿› - ç³»ç»Ÿä¼˜åŒ–æ•ˆæœæœ‰é™ï¼Œéœ€è¦é‡æ–°è¯„ä¼°ä¼˜åŒ–ç­–ç•¥")
        
        report.append("")
        
        # ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’
        report.append("ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’:")
        high_priority_recommendations = self.get_high_priority_recommendations()
        for i, rec in enumerate(high_priority_recommendations[:5], 1):
            report.append(f"  {i}. {rec}")
        
        return "\n".join(report)
    
    def calculate_overall_score(self) -> float:
        """è®¡ç®—æ•´ä½“è¯„åˆ†"""
        if not self.evaluations:
            return 0.0
        
        scores = []
        
        for eval_result in self.evaluations:
            component_score = 0.0
            
            # å“åº”æ—¶é—´æ”¹è¿› (25%)
            response_improvement = eval_result.improvement_percentage.get('response_time', 0)
            response_score = min(25, max(0, response_improvement / 2))  # 50%æ”¹è¿›å¾—æ»¡åˆ†
            component_score += response_score
            
            # ååé‡æ”¹è¿› (25%)
            throughput_improvement = eval_result.improvement_percentage.get('throughput', 0)
            throughput_score = min(25, max(0, throughput_improvement / 4))  # 100%æ”¹è¿›å¾—æ»¡åˆ†
            component_score += throughput_score
            
            # æˆåŠŸç‡ (20%)
            success_rate = eval_result.after_optimization.success_rate
            success_score = min(20, max(0, (success_rate - 90) * 2))  # 95%ä»¥ä¸Šå¾—æ»¡åˆ†
            component_score += success_score
            
            # èµ„æºä½¿ç”¨ä¼˜åŒ– (20%)
            cpu_improvement = eval_result.improvement_percentage.get('cpu_usage', 0)
            memory_improvement = eval_result.improvement_percentage.get('memory_usage', 0)
            resource_score = min(20, max(0, (cpu_improvement + memory_improvement) / 4))
            component_score += resource_score
            
            # ç“¶é¢ˆè§£å†³æƒ…å†µ (10%)
            bottleneck_score = max(0, 10 - len(eval_result.bottlenecks_identified) * 2)
            component_score += bottleneck_score
            
            scores.append(component_score)
        
        return statistics.mean(scores)
    
    def get_high_priority_recommendations(self) -> List[str]:
        """è·å–é«˜ä¼˜å…ˆçº§å»ºè®®"""
        all_recommendations = []
        recommendation_count = {}
        
        for eval_result in self.evaluations:
            for rec in eval_result.recommendations:
                all_recommendations.append(rec)
                recommendation_count[rec] = recommendation_count.get(rec, 0) + 1
        
        # æŒ‰å‡ºç°é¢‘ç‡æ’åº
        sorted_recommendations = sorted(
            recommendation_count.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        return [rec for rec, count in sorted_recommendations]
    
    def save_results(self, output_file: str = "component_evaluation_results.json"):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        results_data = []
        for eval_result in self.evaluations:
            results_data.append({
                "component": eval_result.component.value,
                "test_name": eval_result.test_name,
                "before_optimization": asdict(eval_result.before_optimization),
                "after_optimization": asdict(eval_result.after_optimization),
                "improvement_percentage": eval_result.improvement_percentage,
                "bottlenecks_identified": eval_result.bottlenecks_identified,
                "optimization_applied": eval_result.optimization_applied,
                "recommendations": eval_result.recommendations,
                "test_duration": eval_result.test_duration,
                "timestamp": eval_result.timestamp
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def create_performance_charts(self, output_dir: str = "charts"):
        """åˆ›å»ºæ€§èƒ½å›¾è¡¨"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        # æ€§èƒ½å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ç»„ä»¶æ€§èƒ½ä¼˜åŒ–å¯¹æ¯”', fontsize=16, fontweight='bold')
        
        components = [eval_result.component.value for eval_result in self.evaluations]
        
        # å“åº”æ—¶é—´å¯¹æ¯”
        before_response = [eval_result.before_optimization.response_time_ms for eval_result in self.evaluations]
        after_response = [eval_result.after_optimization.response_time_ms for eval_result in self.evaluations]
        
        x = np.arange(len(components))
        width = 0.35
        
        axes[0, 0].bar(x - width/2, before_response, width, label='ä¼˜åŒ–å‰', alpha=0.8)
        axes[0, 0].bar(x + width/2, after_response, width, label='ä¼˜åŒ–å', alpha=0.8)
        axes[0, 0].set_xlabel('ç»„ä»¶')
        axes[0, 0].set_ylabel('å“åº”æ—¶é—´ (ms)')
        axes[0, 0].set_title('å“åº”æ—¶é—´å¯¹æ¯”')
        axes[0, 0].set_xticks(x)
        axes[0, 0].set_xticklabels(components)
        axes[0, 0].legend()
        
        # ååé‡å¯¹æ¯”
        before_throughput = [eval_result.before_optimization.throughput_qps for eval_result in self.evaluations]
        after_throughput = [eval_result.after_optimization.throughput_qps for eval_result in self.evaluations]
        
        axes[0, 1].bar(x - width/2, before_throughput, width, label='ä¼˜åŒ–å‰', alpha=0.8)
        axes[0, 1].bar(x + width/2, after_throughput, width, label='ä¼˜åŒ–å', alpha=0.8)
        axes[0, 1].set_xlabel('ç»„ä»¶')
        axes[0, 1].set_ylabel('ååé‡ (QPS)')
        axes[0, 1].set_title('ååé‡å¯¹æ¯”')
        axes[0, 1].set_xticks(x)
        axes[0, 1].set_xticklabels(components)
        axes[0, 1].legend()
        
        # æˆåŠŸç‡å¯¹æ¯”
        before_success = [eval_result.before_optimization.success_rate for eval_result in self.evaluations]
        after_success = [eval_result.after_optimization.success_rate for eval_result in self.evaluations]
        
        axes[1, 0].bar(x - width/2, before_success, width, label='ä¼˜åŒ–å‰', alpha=0.8)
        axes[1, 0].bar(x + width/2, after_success, width, label='ä¼˜åŒ–å', alpha=0.8)
        axes[1, 0].set_xlabel('ç»„ä»¶')
        axes[1, 0].set_ylabel('æˆåŠŸç‡ (%)')
        axes[1, 0].set_title('æˆåŠŸç‡å¯¹æ¯”')
        axes[1, 0].set_xticks(x)
        axes[1, 0].set_xticklabels(components)
        axes[1, 0].legend()
        
        # èµ„æºä½¿ç”¨å¯¹æ¯”
        before_cpu = [eval_result.before_optimization.cpu_usage for eval_result in self.evaluations]
        after_cpu = [eval_result.after_optimization.cpu_usage for eval_result in self.evaluations]
        
        axes[1, 1].bar(x - width/2, before_cpu, width, label='ä¼˜åŒ–å‰', alpha=0.8)
        axes[1, 1].bar(x + width/2, after_cpu, width, label='ä¼˜åŒ–å', alpha=0.8)
        axes[1, 1].set_xlabel('ç»„ä»¶')
        axes[1, 1].set_ylabel('CPUä½¿ç”¨ç‡')
        axes[1, 1].set_title('CPUä½¿ç”¨ç‡å¯¹æ¯”')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(components)
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/performance_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # æ”¹è¿›ç™¾åˆ†æ¯”é›·è¾¾å›¾
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        metrics = ['å“åº”æ—¶é—´', 'ååé‡', 'æˆåŠŸç‡', 'CPUä½¿ç”¨', 'å†…å­˜ä½¿ç”¨']
        
        for eval_result in self.evaluations:
            values = [
                eval_result.improvement_percentage.get('response_time', 0),
                eval_result.improvement_percentage.get('throughput', 0),
                eval_result.improvement_percentage.get('success_rate', 0),
                eval_result.improvement_percentage.get('cpu_usage', 0),
                eval_result.improvement_percentage.get('memory_usage', 0)
            ]
            
            # é—­åˆé›·è¾¾å›¾
            values += values[:1]
            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
            angles += angles[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=eval_result.component.value)
            ax.fill(angles, values, alpha=0.25)
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_ylim(-20, 100)
        ax.set_title('ç»„ä»¶ä¼˜åŒ–æ”¹è¿›ç™¾åˆ†æ¯”', size=16, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax.grid(True)
        
        plt.savefig(f"{output_dir}/improvement_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}/")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Xiaozhi ESP32 Server ç»„ä»¶è¯„ä¼°")
    parser.add_argument("--url", default="http://localhost:8080", help="æœåŠ¡å™¨URL")
    parser.add_argument("--config", default="optimization-configs.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--duration", type=int, default=30, help="æ¯ä¸ªæµ‹è¯•çš„æŒç»­æ—¶é—´(ç§’)")
    parser.add_argument("--components", nargs='+', choices=['VAD', 'ASR', 'LLM', 'TTS'], 
                       default=['VAD', 'ASR', 'LLM', 'TTS'], help="è¦è¯„ä¼°çš„ç»„ä»¶")
    parser.add_argument("--output", default="component_evaluation_results.json", help="ç»“æœè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--report", default="component_evaluation_report.txt", help="æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶")
    parser.add_argument("--charts", action="store_true", help="ç”Ÿæˆæ€§èƒ½å›¾è¡¨")
    
    args = parser.parse_args()
    
    async with ComponentEvaluator(args.url, args.config) as evaluator:
        try:
            evaluator.load_config()
            
            # è¯„ä¼°æŒ‡å®šç»„ä»¶
            for component_name in args.components:
                component = ComponentType(component_name)
                await evaluator.evaluate_component(component, args.duration)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = evaluator.generate_evaluation_report()
            
            # ä¿å­˜ç»“æœ
            evaluator.save_results(args.output)
            
            # ä¿å­˜æŠ¥å‘Š
            with open(args.report, 'w', encoding='utf-8') as f:
                f.write(report)
            
            # ç”Ÿæˆå›¾è¡¨
            if args.charts:
                evaluator.create_performance_charts()
            
            # æ‰“å°æŠ¥å‘Š
            print(report)
            
            logger.info("ç»„ä»¶è¯„ä¼°å®Œæˆ")
            
        except KeyboardInterrupt:
            logger.info("è¯„ä¼°è¢«ç”¨æˆ·ä¸­æ–­")
            exit(130)
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            exit(1)

if __name__ == "__main__":
    asyncio.run(main())
#!/usr/bin/env python3
"""
ASR (Automatic Speech Recognition) å¾®æœåŠ¡
åŸºäºSenseVoiceï¼Œæ”¯æŒæ‰¹å¤„ç†ã€é˜Ÿåˆ—ç®¡ç†å’Œæ¨¡å‹ç¼“å­˜ä¼˜åŒ–
4æ ¸8GBæœåŠ¡å™¨æé™ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ”¯æŒ80-100å°è®¾å¤‡å¹¶å‘
"""

import sys
import os
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import logging
import time
import hashlib
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import numpy as np
import torch
import redis.asyncio as redis
from redis.asyncio.connection import ConnectionPool

try:
    from config.redis_config import get_redis_client, OptimizedRedisClient
    from core.queue_manager import get_queue_manager, QueueRequest, Priority
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„æ›¿ä»£å®ç°
    logger = logging.getLogger(__name__)
    logger.warning("æ— æ³•å¯¼å…¥Rediså’Œé˜Ÿåˆ—ç®¡ç†æ¨¡å—ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬")
    
    class OptimizedRedisClient:
        async def get(self, key): return None
        async def set(self, key, value, ex=None): pass
        async def close(self): pass
    
    def get_redis_client():
        return OptimizedRedisClient()
    
    class QueueRequest:
        def __init__(self, **kwargs): pass
    
    class Priority:
        HIGH = 1
        MEDIUM = 2
        LOW = 3
    
    def get_queue_manager():
        return None
from fastapi import FastAPI, WebSocket, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import json
import base64
from funasr import AutoModel
import librosa
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ASRRequest:
    session_id: str
    audio_data: bytes
    sample_rate: int = 16000
    language: str = "zh"
    timestamp: float = 0.0
    priority: int = 1  # 1=é«˜ä¼˜å…ˆçº§, 2=ä¸­ä¼˜å…ˆçº§, 3=ä½ä¼˜å…ˆçº§

@dataclass
class ASRResult:
    session_id: str
    text: str
    confidence: float
    language: str
    timestamp: float
    processing_time: float
    cached: bool = False

class SenseVoiceProcessor:
    """4æ ¸8GBæœåŠ¡å™¨æé™ä¼˜åŒ–çš„SenseVoiceå¤„ç†å™¨ï¼Œæ”¯æŒæ‰¹å¤„ç†å’Œé‡åŒ–ä¼˜åŒ–"""
    
    def __init__(self, model_dir: str = "models/SenseVoiceSmall", batch_size: int = None, enable_fp16: bool = True):
        self.model_dir = model_dir
        # æé™ä¼˜åŒ–ï¼šä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®ï¼Œé»˜è®¤å¤§å¹…æå‡
        self.batch_size = batch_size or int(os.getenv("ASR_BATCH_SIZE", "32"))  # æå‡åˆ°32
        self.enable_fp16 = enable_fp16
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # æé™ä¼˜åŒ–ï¼šå®æ—¶å¤„ç†å‚æ•°
        self.chunk_size = 512  # 32ms@16kHzï¼Œè¿›ä¸€æ­¥é™ä½å»¶è¿Ÿ
        self.chunk_duration_ms = 32
        self.overlap_ms = 4  # æœ€å°é‡å 
        self.max_audio_length_s = 20  # é™ä½æœ€å¤§éŸ³é¢‘é•¿åº¦
        self.beam_size = 1  # è´ªå©ªè§£ç ï¼Œæœ€å¿«é€Ÿåº¦
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_requests = 0
        self.total_processing_time = 0.0
        self.cache_hits = 0
        
        # å†…å­˜ç®¡ç† - æé™ä¼˜åŒ–
        self.max_memory_mb = int(os.getenv("ASR_MEMORY_LIMIT", "10240"))  # 10GBå†…å­˜é™åˆ¶
        self.audio_buffer_pool = []
        self.result_cache_max_size = int(os.getenv("ASR_CACHE_SIZE_MB", "6144"))  # 6GBç¼“å­˜
        
        # æé™ä¼˜åŒ–ï¼šå¯ç”¨æ‰€æœ‰æ€§èƒ½ç‰¹æ€§
        self.enable_turbo = os.getenv("ASR_ENABLE_TURBO", "true").lower() == "true"
        self.enable_memory_pool = os.getenv("ASR_MEMORY_POOL", "true").lower() == "true"
        self.enable_zero_copy = os.getenv("ASR_ZERO_COPY", "true").lower() == "true"
        self.enable_int8 = os.getenv("ASR_ENABLE_INT8", "true").lower() == "true"
        self.enable_fp16 = os.getenv("ASR_ENABLE_FP16", "true").lower() == "true"
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        self.warmup_model()
    
    def _model_exists(self, model_path: str) -> bool:
        """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
        return os.path.exists(model_path)
    
    def load_model(self):
        """åŠ è½½SenseVoiceæ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨é‡åŒ–ç‰ˆæœ¬"""
        try:
            # æé™ä¼˜åŒ–ï¼šä¼˜å…ˆåŠ è½½æœ€é«˜æ€§èƒ½é‡åŒ–æ¨¡å‹
            model_candidates = []
            
            if self.enable_int8:
                model_candidates.append(f"{self.model_dir}_int8")
            if self.enable_fp16:
                model_candidates.append(f"{self.model_dir}_fp16")
            model_candidates.append(self.model_dir)
            
            model_path = None
            for candidate in model_candidates:
                if candidate and self._model_exists(candidate):
                    model_path = candidate
                    break
            
            if not model_path:
                raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {self.model_dir}")
            
            logger.info(f"ğŸš€ åŠ è½½ASRæ¨¡å‹: {model_path}")
            
            # æé™ä¼˜åŒ–ï¼šæ¨¡å‹åŠ è½½é…ç½®
            model_config = {
                "model": model_path,
                "device": self.device,
                "batch_size": self.batch_size,
                "disable_update": True,  # ç¦ç”¨æ¨¡å‹æ›´æ–°ä»¥æå‡æ€§èƒ½
                "disable_log": True,     # ç¦ç”¨æ—¥å¿—ä»¥æå‡æ€§èƒ½
            }
            
            if self.enable_fp16 and torch.cuda.is_available():
                model_config["dtype"] = torch.float16
            
            self.model = AutoModel(**model_config)
            
            # æé™ä¼˜åŒ–ï¼šæ¨¡å‹ç¼–è¯‘åŠ é€Ÿ
            if hasattr(torch, 'compile') and self.enable_turbo:
                try:
                    self.model = torch.compile(self.model, mode="max-autotune")
                    logger.info("âœ… å¯ç”¨Torchç¼–è¯‘åŠ é€Ÿ")
                except Exception as e:
                    logger.warning(f"Torchç¼–è¯‘å¤±è´¥: {e}")
            
            logger.info(f"âœ… ASRæ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ‰¹å¤„ç†å¤§å°: {self.batch_size}")
            
        except Exception as e:
            logger.error(f"âŒ ASRæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def warmup_model(self):
        """æ¨¡å‹é¢„çƒ­ï¼Œä¼˜åŒ–é¦–æ¬¡æ¨ç†æ€§èƒ½"""
        try:
            logger.info("ğŸ”¥ ASRæ¨¡å‹é¢„çƒ­ä¸­...")
            
            # åˆ›å»ºé¢„çƒ­éŸ³é¢‘æ•°æ® - æé™ä¼˜åŒ–ï¼šæ›´å°çš„é¢„çƒ­æ•°æ®
            warmup_audio = np.random.randn(8000).astype(np.float32)  # 0.5ç§’éŸ³é¢‘
            
            # æ‰¹é‡é¢„çƒ­ - æé™ä¼˜åŒ–ï¼šé¢„çƒ­æ›´å¤§æ‰¹æ¬¡
            warmup_batch = [warmup_audio] * min(self.batch_size, 16)
            
            start_time = time.time()
            _ = self.model.generate(input=warmup_batch, batch_size=len(warmup_batch))
            warmup_time = time.time() - start_time
            
            logger.info(f"âœ… ASRæ¨¡å‹é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {warmup_time:.2f}s")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ASRæ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")

    def preprocess_audio(self, audio_data: bytes, sample_rate: int = 16000) -> np.ndarray:
        """éŸ³é¢‘é¢„å¤„ç†ï¼Œ4æ ¸8GBä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºnumpyæ•°ç»„
            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            
            # é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
            if sample_rate != 16000:
                audio_array = librosa.resample(audio_array, orig_sr=sample_rate, target_sr=16000)
            
            # 4æ ¸8GBä¼˜åŒ–ï¼šé™åˆ¶éŸ³é¢‘é•¿åº¦ä»¥èŠ‚çœå†…å­˜
            max_samples = 16000 * self.max_audio_length_s
            if len(audio_array) > max_samples:
                audio_array = audio_array[:max_samples]
            
            return audio_array
        except Exception as e:
            logger.error(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
            raise
    
    def generate_audio_hash(self, audio_data: bytes) -> str:
        """ç”ŸæˆéŸ³é¢‘æ•°æ®çš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜"""
        return hashlib.md5(audio_data).hexdigest()
    
    async def process_batch(self, requests: List[ASRRequest]) -> List[ASRResult]:
        """æ‰¹å¤„ç†ASRè¯·æ±‚ï¼Œ4æ ¸8GBä¼˜åŒ–ç‰ˆæœ¬"""
        if not requests:
            return []
        
        start_time = time.time()
        results = []
        
        try:
            # 4æ ¸8GBä¼˜åŒ–ï¼šé™åˆ¶æ‰¹å¤„ç†å¤§å°
            actual_batch_size = min(len(requests), self.batch_size)
            batch_requests = requests[:actual_batch_size]
            
            # é¢„å¤„ç†éŸ³é¢‘æ•°æ®
            audio_inputs = []
            for req in batch_requests:
                try:
                    audio_array = self.preprocess_audio(req.audio_data, req.sample_rate)
                    audio_inputs.append(audio_array)
                except Exception as e:
                    logger.error(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥ {req.session_id}: {e}")
                    # æ·»åŠ é”™è¯¯ç»“æœ
                    results.append(ASRResult(
                        session_id=req.session_id,
                        text="",
                        confidence=0.0,
                        language=req.language,
                        timestamp=req.timestamp,
                        processing_time=0.0,
                        cached=False
                    ))
                    continue
            
            if not audio_inputs:
                return results
            
            # æ‰¹é‡æ¨ç†
            try:
                # 4æ ¸8GBä¼˜åŒ–ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„æ¨ç†å‚æ•°
                batch_results = self.model.generate(
                    input=audio_inputs,
                    cache={},
                    language="zh",
                    use_itn=True,
                    # 4æ ¸8GBä¼˜åŒ–ï¼šç¦ç”¨ä¸€äº›é«˜çº§åŠŸèƒ½ä»¥èŠ‚çœèµ„æº
                    batch_size=len(audio_inputs),
                    # é™ä½beam_sizeä»¥æå‡é€Ÿåº¦
                    beam_size=self.beam_size
                )
                
                # å¤„ç†ç»“æœ
                processing_time = time.time() - start_time
                
                for i, (req, result) in enumerate(zip(batch_requests, batch_results)):
                    if isinstance(result, list) and len(result) > 0:
                        text = result[0].get('text', '') if isinstance(result[0], dict) else str(result[0])
                        confidence = result[0].get('confidence', 0.8) if isinstance(result[0], dict) else 0.8
                    else:
                        text = str(result) if result else ""
                        confidence = 0.8 if text else 0.0
                    
                    asr_result = ASRResult(
                        session_id=req.session_id,
                        text=text,
                        confidence=confidence,
                        language=req.language,
                        timestamp=req.timestamp,
                        processing_time=processing_time / len(batch_requests),
                        cached=False
                    )
                    results.append(asr_result)
                
            except Exception as e:
                logger.error(f"æ‰¹é‡ASRæ¨ç†å¤±è´¥: {e}")
                # ä¸ºæ‰€æœ‰è¯·æ±‚æ·»åŠ é”™è¯¯ç»“æœ
                for req in batch_requests:
                    results.append(ASRResult(
                        session_id=req.session_id,
                        text="",
                        confidence=0.0,
                        language=req.language,
                        timestamp=req.timestamp,
                        processing_time=time.time() - start_time,
                        cached=False
                    ))
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.total_requests += len(batch_requests)
            self.total_processing_time += time.time() - start_time
            
            return results
            
        except Exception as e:
            logger.error(f"æ‰¹å¤„ç†ASRå¤±è´¥: {e}")
            return []
    
    def get_stats(self) -> Dict:
        """è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        avg_time = self.total_processing_time / max(self.total_requests, 1)
        cache_hit_rate = self.cache_hits / max(self.total_requests, 1)
        
        return {
            'total_requests': self.total_requests,
            'avg_processing_time': avg_time,
            'cache_hit_rate': cache_hit_rate,
            'model_device': str(self.device),
            'batch_size': self.batch_size,
            'chunk_size': self.chunk_size
        }

class ASRService:
    """æé™ä¼˜åŒ–çš„ASRæœåŠ¡ï¼Œæ”¯æŒ80-100å°è®¾å¤‡å¹¶å‘"""
    
    def __init__(self, batch_size: int = None, max_concurrent: int = None):
        # æé™ä¼˜åŒ–ï¼šä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.batch_size = batch_size or int(os.getenv("ASR_BATCH_SIZE", "32"))
        self.max_concurrent = max_concurrent or int(os.getenv("ASR_MAX_CONCURRENT", "160"))
        self.batch_timeout = float(os.getenv("ASR_BATCH_TIMEOUT", "50")) / 1000  # 50ms
        self.queue_size = int(os.getenv("ASR_QUEUE_SIZE", "400"))
        self.worker_threads = int(os.getenv("ASR_WORKER_THREADS", "12"))
        self.io_threads = int(os.getenv("ASR_IO_THREADS", "4"))
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.processor = SenseVoiceProcessor(batch_size=self.batch_size)
        self.redis_client = None
        self.request_queue = asyncio.Queue(maxsize=self.queue_size)
        self.result_futures = {}
        self.processing_semaphore = asyncio.Semaphore(self.max_concurrent)
        
        # æé™ä¼˜åŒ–ï¼šçº¿ç¨‹æ± é…ç½®
        self.thread_pool = ThreadPoolExecutor(
            max_workers=self.worker_threads,
            thread_name_prefix="ASR-Worker"
        )
        self.io_pool = ThreadPoolExecutor(
            max_workers=self.io_threads,
            thread_name_prefix="ASR-IO"
        )
        
        # æ€§èƒ½ç›‘æ§
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "cache_hits": 0,
            "average_processing_time": 0.0,
            "current_queue_size": 0,
            "max_concurrent": self.max_concurrent,
            "batch_size": self.batch_size,
        }
        
        # å¯åŠ¨åå°ä»»åŠ¡
        asyncio.create_task(self.batch_processor())
        asyncio.create_task(self.performance_monitor())

    async def batch_processor(self):
        """æé™ä¼˜åŒ–çš„æ‰¹å¤„ç†å™¨"""
        logger.info(f"ğŸš€ å¯åŠ¨ASRæ‰¹å¤„ç†å™¨ï¼Œæ‰¹å¤§å°: {self.batch_size}, è¶…æ—¶: {self.batch_timeout*1000:.0f}ms")
        
        while True:
            try:
                batch_requests = []
                start_time = time.time()
                
                # æé™ä¼˜åŒ–ï¼šåŠ¨æ€æ‰¹å¤„ç†æ”¶é›†
                while len(batch_requests) < self.batch_size:
                    try:
                        remaining_time = self.batch_timeout - (time.time() - start_time)
                        if remaining_time <= 0:
                            break
                        
                        request = await asyncio.wait_for(
                            self.request_queue.get(),
                            timeout=remaining_time
                        )
                        batch_requests.append(request)
                        
                    except asyncio.TimeoutError:
                        break
                
                if batch_requests:
                    # æé™ä¼˜åŒ–ï¼šå¹¶è¡Œå¤„ç†æ‰¹æ¬¡
                    await self._process_batch_parallel(batch_requests)
                else:
                    # æé™ä¼˜åŒ–ï¼šçŸ­æš‚ä¼‘çœ é¿å…CPUç©ºè½¬
                    await asyncio.sleep(0.001)
                    
            except Exception as e:
                logger.error(f"âŒ æ‰¹å¤„ç†å™¨é”™è¯¯: {e}")
                await asyncio.sleep(0.01)

    async def _process_batch_parallel(self, requests: List[ASRRequest]):
        """å¹¶è¡Œå¤„ç†æ‰¹æ¬¡è¯·æ±‚"""
        try:
            # æé™ä¼˜åŒ–ï¼šå¹¶è¡Œç¼“å­˜æ£€æŸ¥
            cache_tasks = [self.get_cached_result(req) for req in requests]
            cached_results = await asyncio.gather(*cache_tasks, return_exceptions=True)
            
            # åˆ†ç¦»ç¼“å­˜å‘½ä¸­å’Œæœªå‘½ä¸­çš„è¯·æ±‚
            uncached_requests = []
            for i, (req, cached) in enumerate(zip(requests, cached_results)):
                if isinstance(cached, ASRResult):
                    # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›ç»“æœ
                    if req.session_id in self.result_futures:
                        self.result_futures[req.session_id].set_result(cached)
                        del self.result_futures[req.session_id]
                    self.stats["cache_hits"] += 1
                else:
                    uncached_requests.append(req)
            
            # å¤„ç†æœªç¼“å­˜çš„è¯·æ±‚
            if uncached_requests:
                results = await self.processor.process_batch(uncached_requests)
                
                # æé™ä¼˜åŒ–ï¼šå¹¶è¡Œç¼“å­˜å­˜å‚¨å’Œç»“æœè¿”å›
                cache_tasks = [self.cache_result(result) for result in results]
                await asyncio.gather(*cache_tasks, return_exceptions=True)
                
                # è¿”å›ç»“æœ
                for result in results:
                    if result.session_id in self.result_futures:
                        self.result_futures[result.session_id].set_result(result)
                        del self.result_futures[result.session_id]
                
                self.stats["successful_requests"] += len(results)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹å¤„ç†å¤±è´¥: {e}")
            # å¤„ç†å¤±è´¥çš„è¯·æ±‚
            for req in requests:
                if req.session_id in self.result_futures:
                    self.result_futures[req.session_id].set_exception(e)
                    del self.result_futures[req.session_id]
            self.stats["failed_requests"] += len(requests)

    async def cache_result(self, result: ASRResult):
        """ç¼“å­˜ASRç»“æœï¼Œ4æ ¸8GBä¼˜åŒ–ç‰ˆæœ¬"""
        if self.redis_client:
            try:
                # ä½¿ç”¨éŸ³é¢‘å“ˆå¸Œä½œä¸ºç¼“å­˜é”®ï¼Œå‡å°‘ç¼“å­˜å¤§å°
                cache_key = f"asr:{result.session_id[:8]}:{int(result.timestamp)}"  # ç®€åŒ–ç¼“å­˜é”®
                cache_data = {
                    'text': result.text,
                    'confidence': result.confidence,
                    'language': result.language,
                    'processing_time': result.processing_time
                }
                # 4æ ¸8GBä¼˜åŒ–ï¼šå‡å°‘ç¼“å­˜æ—¶é—´åˆ°15åˆ†é’Ÿ
                await self.redis_client.setex(cache_key, 900, json.dumps(cache_data))
            except Exception as e:
                logger.warning(f"ç¼“å­˜ASRç»“æœå¤±è´¥: {e}")
    
    async def get_cached_result(self, request: ASRRequest) -> Optional[ASRResult]:
        """ä»ç¼“å­˜è·å–ASRç»“æœï¼Œ4æ ¸8GBä¼˜åŒ–ç‰ˆæœ¬"""
        if self.redis_client:
            try:
                # ä½¿ç”¨éŸ³é¢‘å“ˆå¸Œä½œä¸ºç¼“å­˜é”®
                audio_hash = self.processor.generate_audio_hash(request.audio_data)
                cache_key = f"asr_hash:{audio_hash[:16]}"  # 4æ ¸8GBä¼˜åŒ–ï¼šç¼©çŸ­å“ˆå¸Œé•¿åº¦
                
                cached_data = await self.redis_client.get(cache_key)
                if cached_data:
                    data = json.loads(cached_data)
                    self.processor.cache_hits += 1
                    return ASRResult(
                        session_id=request.session_id,
                        text=data['text'],
                        confidence=data['confidence'],
                        language=data['language'],
                        timestamp=request.timestamp,
                        processing_time=data['processing_time'],
                        cached=True
                    )
            except Exception as e:
                logger.warning(f"è·å–ç¼“å­˜ASRç»“æœå¤±è´¥: {e}")
        return None
    
    async def add_request(self, request: ASRRequest):
        """æ·»åŠ è¯·æ±‚åˆ°ç›¸åº”çš„ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼Œ4æ ¸8GBä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            if request.priority == 1:
                await self.high_priority_queue.put(request)
            elif request.priority == 2:
                await self.medium_priority_queue.put(request)
            else:
                await self.low_priority_queue.put(request)
        except asyncio.QueueFull:
            logger.warning(f"é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒè¯·æ±‚: {request.session_id}")
            raise HTTPException(status_code=503, detail="æœåŠ¡å™¨ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•")

# FastAPIåº”ç”¨
app = FastAPI(title="ASR Service", version="1.0.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€ASRæœåŠ¡å®ä¾‹ - 4æ ¸8GBæ¿€è¿›ä¼˜åŒ–é…ç½®ï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
asr_service = ASRService()

@app.on_event("startup")
async def startup_event():
    await asr_service.initialize()

@app.post("/asr/recognize")
async def recognize_speech(
    session_id: str,
    audio_data: str,  # base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
    sample_rate: int = 16000,
    language: str = "zh",
    priority: int = 2,
    timestamp: float = 0.0,
    background_tasks: BackgroundTasks = None
):
    """è¯­éŸ³è¯†åˆ«APIï¼Œ4æ ¸8GBä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        # è§£ç éŸ³é¢‘æ•°æ®
        audio_bytes = base64.b64decode(audio_data)
        
        # 4æ ¸8GBä¼˜åŒ–ï¼šé™åˆ¶éŸ³é¢‘æ•°æ®å¤§å°
        max_audio_size = 1024 * 1024  # 1MBé™åˆ¶
        if len(audio_bytes) > max_audio_size:
            return {
                "session_id": session_id,
                "text": "",
                "confidence": 0.0,
                "error": "éŸ³é¢‘æ•°æ®è¿‡å¤§ï¼Œè¯·å‹ç¼©åé‡è¯•"
            }
        
        # åˆ›å»ºASRè¯·æ±‚
        request = ASRRequest(
            session_id=session_id,
            audio_data=audio_bytes,
            sample_rate=sample_rate,
            language=language,
            timestamp=timestamp,
            priority=priority
        )
        
        # æ£€æŸ¥ç¼“å­˜
        cached_result = await asr_service.get_cached_result(request)
        if cached_result:
            return {
                "session_id": cached_result.session_id,
                "text": cached_result.text,
                "confidence": cached_result.confidence,
                "language": cached_result.language,
                "timestamp": cached_result.timestamp,
                "processing_time": cached_result.processing_time,
                "cached": True
            }
        
        # æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
        await asr_service.add_request(request)
        
        return {
            "session_id": session_id,
            "status": "processing",
            "message": "è¯·æ±‚å·²æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—"
        }
        
    except Exception as e:
        logger.error(f"ASRè¯†åˆ«å¤±è´¥: {e}")
        return {
            "session_id": session_id,
            "text": "",
            "confidence": 0.0,
            "language": language,
            "timestamp": timestamp,
            "processing_time": 0.0,
            "cached": False,
            "error": str(e)
        }

@app.get("/asr/stats")
async def get_stats():
    """è·å–ASRæœåŠ¡ç»Ÿè®¡ä¿¡æ¯"""
    processor_stats = asr_service.processor.get_stats()
    service_stats = asr_service.performance_stats
    
    return {
        "processor": processor_stats,
        "service": service_stats,
        "queues": {
            "high": asr_service.high_priority_queue.qsize(),
            "medium": asr_service.medium_priority_queue.qsize(),
            "low": asr_service.low_priority_queue.qsize()
        }
    }

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "asr", "optimization": "4core_8gb"}

# æ·»åŠ æµå¼ASRè·¯ç”±ï¼ˆé›†æˆèŠå¤©è®°å½•åŠŸèƒ½ï¼‰
try:
    from asr_streaming_enhancement import add_streaming_routes
    streaming_service = add_streaming_routes(app, asr_service)
    logger.info("âœ… æµå¼ASRè·¯ç”±å·²é›†æˆï¼Œæ”¯æŒèŠå¤©è®°å½•åŠŸèƒ½")
except ImportError as e:
    logger.warning(f"âš ï¸ æ— æ³•å¯¼å…¥æµå¼ASRæ¨¡å—: {e}")
except Exception as e:
    logger.error(f"âŒ é›†æˆæµå¼ASRè·¯ç”±å¤±è´¥: {e}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, workers=1)
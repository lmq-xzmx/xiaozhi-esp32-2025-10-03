#!/usr/bin/env python3
"""
å®æ—¶æ•°æ®åŒæ­¥æœåŠ¡
åœ¨Redisç¼“å­˜å’Œæ•°æ®åº“ä¹‹é—´å®ç°å®æ—¶æ•°æ®åŒæ­¥
"""

import asyncio
import logging
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, asdict
from enum import Enum
import uuid

from config.redis_config import get_redis_client
from core.chat_history_service import ChatHistoryService
from core.enhanced_db_service import get_enhanced_db_service

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SyncOperation(Enum):
    """åŒæ­¥æ“ä½œç±»å‹"""
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    BATCH_UPDATE = "batch_update"


class SyncDirection(Enum):
    """åŒæ­¥æ–¹å‘"""
    CACHE_TO_DB = "cache_to_db"
    DB_TO_CACHE = "db_to_cache"
    BIDIRECTIONAL = "bidirectional"


@dataclass
class SyncTask:
    """åŒæ­¥ä»»åŠ¡"""
    task_id: str
    operation: SyncOperation
    direction: SyncDirection
    table_name: str
    data: Dict[str, Any]
    cache_key: str
    created_at: datetime
    priority: int = 1  # 1=é«˜ä¼˜å…ˆçº§, 2=ä¸­ä¼˜å…ˆçº§, 3=ä½ä¼˜å…ˆçº§
    retry_count: int = 0
    max_retries: int = 3
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "task_id": self.task_id,
            "operation": self.operation.value,
            "direction": self.direction.value,
            "table_name": self.table_name,
            "data": self.data,
            "cache_key": self.cache_key,
            "created_at": self.created_at.isoformat(),
            "priority": self.priority,
            "retry_count": self.retry_count,
            "max_retries": self.max_retries
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SyncTask':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            task_id=data["task_id"],
            operation=SyncOperation(data["operation"]),
            direction=SyncDirection(data["direction"]),
            table_name=data["table_name"],
            data=data["data"],
            cache_key=data["cache_key"],
            created_at=datetime.fromisoformat(data["created_at"]),
            priority=data.get("priority", 1),
            retry_count=data.get("retry_count", 0),
            max_retries=data.get("max_retries", 3)
        )


class RealtimeSyncService:
    """å®æ—¶æ•°æ®åŒæ­¥æœåŠ¡"""
    
    def __init__(self, sync_interval: int = 5, batch_size: int = 100):
        """
        åˆå§‹åŒ–å®æ—¶åŒæ­¥æœåŠ¡
        
        Args:
            sync_interval: åŒæ­¥é—´éš”ï¼ˆç§’ï¼‰
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        self.sync_interval = sync_interval
        self.batch_size = batch_size
        self.redis_client = None
        self.chat_service = None
        self.db_service = None
        self.is_running = False
        
        # é˜Ÿåˆ—é”®
        self.SYNC_QUEUE_KEY = "realtime_sync_queue"
        self.FAILED_QUEUE_KEY = "realtime_sync_failed"
        self.PROCESSING_KEY = "realtime_sync_processing"
        
        # ç¼“å­˜é”®å‰ç¼€
        self.CHAT_RECORD_PREFIX = "chat_record:"
        self.DEVICE_STATS_PREFIX = "device_stats:"
        self.SESSION_PREFIX = "session:"
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_tasks": 0,
            "successful_syncs": 0,
            "failed_syncs": 0,
            "cache_to_db_syncs": 0,
            "db_to_cache_syncs": 0,
            "last_sync_time": None,
            "queue_size": 0,
            "processing_time_avg": 0.0
        }
        
        # åŒæ­¥å¤„ç†å™¨æ˜ å°„
        self.sync_handlers = {
            "chat_records": self._sync_chat_records,
            "device_stats": self._sync_device_stats,
            "sessions": self._sync_sessions
        }
    
    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        try:
            self.redis_client = await get_redis_client()
            self.chat_service = ChatHistoryService()
            self.db_service = get_enhanced_db_service()
            
            logger.info("ğŸš€ å®æ—¶æ•°æ®åŒæ­¥æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"âŒ å®æ—¶æ•°æ®åŒæ­¥æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def add_sync_task(self, operation: SyncOperation, direction: SyncDirection, 
                           table_name: str, data: Dict[str, Any], 
                           cache_key: str, priority: int = 1) -> str:
        """æ·»åŠ åŒæ­¥ä»»åŠ¡"""
        task_id = f"{table_name}_{int(time.time())}_{uuid.uuid4().hex[:8]}"
        
        task = SyncTask(
            task_id=task_id,
            operation=operation,
            direction=direction,
            table_name=table_name,
            data=data,
            cache_key=cache_key,
            created_at=datetime.now(),
            priority=priority
        )
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        await self.redis_client.client.zadd(
            self.SYNC_QUEUE_KEY, 
            {json.dumps(task.to_dict()): priority}
        )
        
        self.stats["total_tasks"] += 1
        self.stats["queue_size"] += 1
        
        logger.debug(f"ğŸ“ æ·»åŠ åŒæ­¥ä»»åŠ¡: {task_id} ({operation.value} -> {direction.value})")
        return task_id
    
    async def sync_chat_record_to_cache(self, record_data: Dict[str, Any]) -> str:
        """åŒæ­¥èŠå¤©è®°å½•åˆ°ç¼“å­˜"""
        cache_key = f"{self.CHAT_RECORD_PREFIX}{record_data.get('id', '')}"
        return await self.add_sync_task(
            operation=SyncOperation.UPDATE,
            direction=SyncDirection.DB_TO_CACHE,
            table_name="chat_records",
            data=record_data,
            cache_key=cache_key,
            priority=1
        )
    
    async def sync_chat_record_to_db(self, record_data: Dict[str, Any]) -> str:
        """åŒæ­¥èŠå¤©è®°å½•åˆ°æ•°æ®åº“"""
        cache_key = f"{self.CHAT_RECORD_PREFIX}{record_data.get('id', '')}"
        return await self.add_sync_task(
            operation=SyncOperation.CREATE,
            direction=SyncDirection.CACHE_TO_DB,
            table_name="chat_records",
            data=record_data,
            cache_key=cache_key,
            priority=1
        )
    
    async def sync_device_stats_to_cache(self, device_id: str, stats_data: Dict[str, Any]) -> str:
        """åŒæ­¥è®¾å¤‡ç»Ÿè®¡åˆ°ç¼“å­˜"""
        cache_key = f"{self.DEVICE_STATS_PREFIX}{device_id}"
        return await self.add_sync_task(
            operation=SyncOperation.UPDATE,
            direction=SyncDirection.DB_TO_CACHE,
            table_name="device_stats",
            data={"device_id": device_id, **stats_data},
            cache_key=cache_key,
            priority=2
        )
    
    async def sync_session_to_cache(self, session_data: Dict[str, Any]) -> str:
        """åŒæ­¥ä¼šè¯åˆ°ç¼“å­˜"""
        cache_key = f"{self.SESSION_PREFIX}{session_data.get('session_id', '')}"
        return await self.add_sync_task(
            operation=SyncOperation.UPDATE,
            direction=SyncDirection.DB_TO_CACHE,
            table_name="sessions",
            data=session_data,
            cache_key=cache_key,
            priority=2
        )
    
    async def start_sync_daemon(self):
        """å¯åŠ¨åŒæ­¥å®ˆæŠ¤è¿›ç¨‹"""
        if self.is_running:
            logger.warning("å®æ—¶åŒæ­¥å®ˆæŠ¤è¿›ç¨‹å·²åœ¨è¿è¡Œ")
            return
        
        self.is_running = True
        logger.info("ğŸš€ å¯åŠ¨å®æ—¶æ•°æ®åŒæ­¥å®ˆæŠ¤è¿›ç¨‹")
        
        # åœ¨åå°å¯åŠ¨åŒæ­¥å¾ªç¯
        asyncio.create_task(self._sync_daemon_loop())
    
    async def _sync_daemon_loop(self):
        """åŒæ­¥å®ˆæŠ¤è¿›ç¨‹å¾ªç¯"""
        while self.is_running:
            try:
                await self._process_sync_queue()
                await asyncio.sleep(self.sync_interval)
            except Exception as e:
                logger.error(f"âŒ åŒæ­¥å®ˆæŠ¤è¿›ç¨‹é”™è¯¯: {e}")
                await asyncio.sleep(10)  # å¤±è´¥åç­‰å¾…10ç§’é‡è¯•
    
    async def stop_sync_daemon(self):
        """åœæ­¢åŒæ­¥å®ˆæŠ¤è¿›ç¨‹"""
        self.is_running = False
        logger.info("ğŸ›‘ åœæ­¢å®æ—¶æ•°æ®åŒæ­¥å®ˆæŠ¤è¿›ç¨‹")
    
    async def _process_sync_queue(self):
        """å¤„ç†åŒæ­¥é˜Ÿåˆ—"""
        try:
            # è·å–é˜Ÿåˆ—å¤§å°
            queue_size = await self.redis_client.client.zcard(self.SYNC_QUEUE_KEY)
            self.stats["queue_size"] = queue_size
            
            if queue_size == 0:
                return
            
            # æ‰¹é‡è·å–ä»»åŠ¡ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            tasks_data = await self.redis_client.client.zrange(
                self.SYNC_QUEUE_KEY, 0, self.batch_size - 1, withscores=True
            )
            
            if not tasks_data:
                return
            
            logger.info(f"ğŸ”„ å¤„ç† {len(tasks_data)} ä¸ªåŒæ­¥ä»»åŠ¡...")
            
            processed_tasks = []
            start_time = time.time()
            
            for task_json, priority in tasks_data:
                try:
                    task_data = json.loads(task_json)
                    task = SyncTask.from_dict(task_data)
                    
                    # å¤„ç†ä»»åŠ¡
                    success = await self._process_sync_task(task)
                    
                    if success:
                        self.stats["successful_syncs"] += 1
                        processed_tasks.append(task_json)
                    else:
                        # å¢åŠ é‡è¯•æ¬¡æ•°
                        task.retry_count += 1
                        if task.retry_count >= task.max_retries:
                            # ç§»åˆ°å¤±è´¥é˜Ÿåˆ—
                            await self.redis_client.client.lpush(
                                self.FAILED_QUEUE_KEY, 
                                json.dumps(task.to_dict())
                            )
                            processed_tasks.append(task_json)
                            self.stats["failed_syncs"] += 1
                            logger.error(f"âŒ ä»»åŠ¡å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {task.task_id}")
                        else:
                            # é‡æ–°åŠ å…¥é˜Ÿåˆ—
                            await self.redis_client.client.zadd(
                                self.SYNC_QUEUE_KEY,
                                {json.dumps(task.to_dict()): task.priority + task.retry_count}
                            )
                            processed_tasks.append(task_json)
                            logger.warning(f"âš ï¸ ä»»åŠ¡é‡è¯• ({task.retry_count}/{task.max_retries}): {task.task_id}")
                
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                    processed_tasks.append(task_json)
            
            # ä»é˜Ÿåˆ—ä¸­ç§»é™¤å·²å¤„ç†çš„ä»»åŠ¡
            if processed_tasks:
                await self.redis_client.client.zrem(self.SYNC_QUEUE_KEY, *processed_tasks)
            
            # æ›´æ–°ç»Ÿè®¡
            processing_time = time.time() - start_time
            self.stats["processing_time_avg"] = (
                self.stats["processing_time_avg"] * 0.8 + processing_time * 0.2
            )
            self.stats["last_sync_time"] = datetime.now().isoformat()
            
            logger.info(f"âœ… å¤„ç†å®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}s")
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†åŒæ­¥é˜Ÿåˆ—å¤±è´¥: {e}")
    
    async def _process_sync_task(self, task: SyncTask) -> bool:
        """å¤„ç†å•ä¸ªåŒæ­¥ä»»åŠ¡"""
        try:
            handler = self.sync_handlers.get(task.table_name)
            if not handler:
                logger.error(f"âŒ æœªæ‰¾åˆ°å¤„ç†å™¨: {task.table_name}")
                return False
            
            return await handler(task)
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†åŒæ­¥ä»»åŠ¡å¤±è´¥ {task.task_id}: {e}")
            return False
    
    async def _sync_chat_records(self, task: SyncTask) -> bool:
        """åŒæ­¥èŠå¤©è®°å½•"""
        try:
            if task.direction == SyncDirection.CACHE_TO_DB:
                # ç¼“å­˜åˆ°æ•°æ®åº“
                if task.operation == SyncOperation.CREATE:
                    # æ’å…¥æ–°è®°å½•
                    await self.chat_service.write_chat_record(
                        device_id=task.data.get("device_id"),
                        session_id=task.data.get("session_id"),
                        message_type=task.data.get("message_type"),
                        content=task.data.get("content"),
                        metadata=task.data.get("metadata", {})
                    )
                elif task.operation == SyncOperation.UPDATE:
                    # æ›´æ–°è®°å½•ï¼ˆå¦‚æœæ”¯æŒï¼‰
                    pass
                
                self.stats["cache_to_db_syncs"] += 1
                
            elif task.direction == SyncDirection.DB_TO_CACHE:
                # æ•°æ®åº“åˆ°ç¼“å­˜
                await self.redis_client.set_with_ttl(
                    task.cache_key,
                    task.data,
                    ttl=3600  # 1å°æ—¶
                )
                
                self.stats["db_to_cache_syncs"] += 1
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥èŠå¤©è®°å½•å¤±è´¥: {e}")
            return False
    
    async def _sync_device_stats(self, task: SyncTask) -> bool:
        """åŒæ­¥è®¾å¤‡ç»Ÿè®¡"""
        try:
            if task.direction == SyncDirection.DB_TO_CACHE:
                # æ•°æ®åº“åˆ°ç¼“å­˜
                await self.redis_client.set_with_ttl(
                    task.cache_key,
                    task.data,
                    ttl=1800  # 30åˆ†é’Ÿ
                )
                
                self.stats["db_to_cache_syncs"] += 1
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥è®¾å¤‡ç»Ÿè®¡å¤±è´¥: {e}")
            return False
    
    async def _sync_sessions(self, task: SyncTask) -> bool:
        """åŒæ­¥ä¼šè¯æ•°æ®"""
        try:
            if task.direction == SyncDirection.DB_TO_CACHE:
                # æ•°æ®åº“åˆ°ç¼“å­˜
                await self.redis_client.set_with_ttl(
                    task.cache_key,
                    task.data,
                    ttl=3600  # 1å°æ—¶
                )
                
                self.stats["db_to_cache_syncs"] += 1
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ä¼šè¯æ•°æ®å¤±è´¥: {e}")
            return False
    
    async def get_queue_status(self) -> Dict[str, Any]:
        """è·å–é˜Ÿåˆ—çŠ¶æ€"""
        try:
            queue_size = await self.redis_client.client.zcard(self.SYNC_QUEUE_KEY)
            failed_size = await self.redis_client.client.llen(self.FAILED_QUEUE_KEY)
            processing_size = await self.redis_client.client.llen(self.PROCESSING_KEY)
            
            return {
                "queue_size": queue_size,
                "failed_size": failed_size,
                "processing_size": processing_size,
                "is_running": self.is_running
            }
        except Exception as e:
            logger.error(f"âŒ è·å–é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def get_failed_tasks(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–å¤±è´¥çš„ä»»åŠ¡"""
        try:
            failed_tasks_json = await self.redis_client.client.lrange(
                self.FAILED_QUEUE_KEY, 0, limit - 1
            )
            
            failed_tasks = []
            for task_json in failed_tasks_json:
                task_data = json.loads(task_json)
                failed_tasks.append(task_data)
            
            return failed_tasks
        except Exception as e:
            logger.error(f"âŒ è·å–å¤±è´¥ä»»åŠ¡å¤±è´¥: {e}")
            return []
    
    async def retry_failed_tasks(self) -> int:
        """é‡è¯•å¤±è´¥çš„ä»»åŠ¡"""
        try:
            failed_tasks_json = await self.redis_client.client.lrange(
                self.FAILED_QUEUE_KEY, 0, -1
            )
            
            retry_count = 0
            for task_json in failed_tasks_json:
                task_data = json.loads(task_json)
                task = SyncTask.from_dict(task_data)
                
                # é‡ç½®é‡è¯•æ¬¡æ•°
                task.retry_count = 0
                
                # é‡æ–°åŠ å…¥é˜Ÿåˆ—
                await self.redis_client.client.zadd(
                    self.SYNC_QUEUE_KEY,
                    {json.dumps(task.to_dict()): task.priority}
                )
                retry_count += 1
            
            # æ¸…ç©ºå¤±è´¥é˜Ÿåˆ—
            await self.redis_client.client.delete(self.FAILED_QUEUE_KEY)
            
            logger.info(f"ğŸ”„ é‡è¯• {retry_count} ä¸ªå¤±è´¥ä»»åŠ¡")
            return retry_count
            
        except Exception as e:
            logger.error(f"âŒ é‡è¯•å¤±è´¥ä»»åŠ¡å¤±è´¥: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "service_status": "running" if self.is_running else "stopped",
            "sync_interval": self.sync_interval,
            "batch_size": self.batch_size,
            "stats": self.stats
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            # æ£€æŸ¥Redisè¿æ¥
            await self.redis_client.client.ping()
            
            # è·å–é˜Ÿåˆ—çŠ¶æ€
            queue_status = await self.get_queue_status()
            
            return {
                "status": "healthy",
                "redis_connected": True,
                "sync_daemon_running": self.is_running,
                "queue_status": queue_status,
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }


# å…¨å±€å®æ—¶åŒæ­¥æœåŠ¡å®ä¾‹
_realtime_sync_service = None


def get_realtime_sync_service() -> RealtimeSyncService:
    """è·å–å…¨å±€å®æ—¶åŒæ­¥æœåŠ¡å®ä¾‹"""
    global _realtime_sync_service
    if _realtime_sync_service is None:
        _realtime_sync_service = RealtimeSyncService()
    return _realtime_sync_service


async def start_realtime_sync_daemon():
    """å¯åŠ¨å®æ—¶åŒæ­¥å®ˆæŠ¤è¿›ç¨‹"""
    service = get_realtime_sync_service()
    await service.initialize()
    await service.start_sync_daemon()


async def stop_realtime_sync_daemon():
    """åœæ­¢å®æ—¶åŒæ­¥å®ˆæŠ¤è¿›ç¨‹"""
    service = get_realtime_sync_service()
    await service.stop_sync_daemon()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_realtime_sync():
        service = RealtimeSyncService()
        await service.initialize()
        
        # æ·»åŠ æµ‹è¯•ä»»åŠ¡
        task_id = await service.sync_chat_record_to_cache({
            "id": "test_record_001",
            "device_id": "test_device_001",
            "session_id": "test_session_001",
            "message_type": "user",
            "content": "æµ‹è¯•æ¶ˆæ¯",
            "created_at": datetime.now().isoformat()
        })
        
        print(f"æ·»åŠ ä»»åŠ¡: {task_id}")
        
        # è·å–é˜Ÿåˆ—çŠ¶æ€
        status = await service.get_queue_status()
        print(f"é˜Ÿåˆ—çŠ¶æ€: {status}")
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = service.get_stats()
        print(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
    
    asyncio.run(test_realtime_sync())
#!/usr/bin/env python3
"""
LLMé¦–Tokenå»¶è¿Ÿä¼˜åŒ–é…ç½®
é’ˆå¯¹é¦–å­—å»¶è¿Ÿæœ€å¤§ç“¶é¢ˆè¿›è¡Œä¼˜åŒ–
"""

import os
import yaml
from typing import Dict, Any

class LLMFirstTokenOptimizer:
    """LLMé¦–Tokenå»¶è¿Ÿä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_configs = {
            # åŸºç¡€ä¼˜åŒ–é…ç½®
            'basic_optimization': {
                'model_settings': {
                    'max_new_tokens': 1,  # é¦–æ¬¡åªç”Ÿæˆ1ä¸ªtoken
                    'do_sample': False,   # å…³é—­é‡‡æ ·ï¼Œä½¿ç”¨è´ªå¿ƒè§£ç 
                    'temperature': 0.1,   # é™ä½æ¸©åº¦ï¼Œå‡å°‘è®¡ç®—
                    'top_p': 0.8,        # å‡å°‘å€™é€‰tokenæ•°é‡
                    'repetition_penalty': 1.0,  # å…³é—­é‡å¤æƒ©ç½š
                },
                'inference_settings': {
                    'use_cache': True,    # å¯ç”¨KVç¼“å­˜
                    'pad_token_id': 0,    # è®¾ç½®padding token
                    'eos_token_id': 2,    # è®¾ç½®ç»“æŸtoken
                    'batch_size': 1,      # é¦–tokenç”Ÿæˆä½¿ç”¨å•batch
                },
                'memory_optimization': {
                    'torch_compile': True,      # å¯ç”¨PyTorchç¼–è¯‘ä¼˜åŒ–
                    'flash_attention': True,    # å¯ç”¨Flash Attention
                    'gradient_checkpointing': False,  # æ¨ç†æ—¶å…³é—­æ¢¯åº¦æ£€æŸ¥ç‚¹
                }
            },
            
            # æ¿€è¿›ä¼˜åŒ–é…ç½®
            'aggressive_optimization': {
                'model_settings': {
                    'max_new_tokens': 1,
                    'do_sample': False,
                    'temperature': 0.0,   # å®Œå…¨ç¡®å®šæ€§
                    'top_k': 1,          # åªè€ƒè™‘æœ€å¯èƒ½çš„token
                    'early_stopping': True,
                },
                'inference_settings': {
                    'use_cache': True,
                    'past_key_values': None,  # é¢„åˆ†é…KVç¼“å­˜
                    'attention_mask': None,   # é¢„è®¡ç®—attention mask
                    'position_ids': None,     # é¢„è®¡ç®—position ids
                },
                'hardware_optimization': {
                    'device_map': 'auto',     # è‡ªåŠ¨è®¾å¤‡æ˜ å°„
                    'torch_dtype': 'float16', # ä½¿ç”¨FP16
                    'low_cpu_mem_usage': True,
                    'offload_folder': '/tmp/llm_offload',
                }
            },
            
            # æµå¼ä¼˜åŒ–é…ç½®
            'streaming_optimization': {
                'streaming_settings': {
                    'stream': True,           # å¯ç”¨æµå¼ç”Ÿæˆ
                    'stream_first_token': True,  # ä¼˜å…ˆç”Ÿæˆé¦–token
                    'chunk_size': 1,          # å•tokenæµå¼è¾“å‡º
                    'buffer_size': 0,         # æ— ç¼“å†²
                },
                'pipeline_settings': {
                    'prefill_optimization': True,   # é¢„å¡«å……ä¼˜åŒ–
                    'speculative_decoding': True,   # æŠ•æœºè§£ç 
                    'parallel_sampling': False,     # å…³é—­å¹¶è¡Œé‡‡æ ·
                }
            }
        }
    
    def generate_llm_config(self, optimization_level: str = 'aggressive') -> Dict[str, Any]:
        """ç”ŸæˆLLMä¼˜åŒ–é…ç½®"""
        base_config = {
            'llm_service': {
                'provider': 'openai',  # æˆ–å…¶ä»–provider
                'model_name': 'gpt-3.5-turbo',
                'api_base': 'http://localhost:8002',
                'timeout': 5.0,  # å‡å°‘è¶…æ—¶æ—¶é—´
                'max_retries': 1,  # å‡å°‘é‡è¯•æ¬¡æ•°
            },
            
            'performance_settings': {
                'max_concurrent_requests': 50,
                'request_queue_size': 100,
                'worker_threads': 8,
                'connection_pool_size': 20,
            },
            
            'caching': {
                'enable_response_cache': True,
                'cache_ttl': 3600,  # 1å°æ—¶ç¼“å­˜
                'cache_size_mb': 512,
                'enable_prompt_cache': True,  # å¯ç”¨promptç¼“å­˜
            },
            
            'first_token_optimization': self.optimization_configs.get(
                optimization_level, 
                self.optimization_configs['basic_optimization']
            )
        }
        
        return base_config
    
    def generate_environment_variables(self, optimization_level: str = 'aggressive') -> Dict[str, str]:
        """ç”Ÿæˆç¯å¢ƒå˜é‡é…ç½®"""
        env_vars = {
            # LLMåŸºç¡€é…ç½®
            'LLM_MAX_CONCURRENT': '50',
            'LLM_TIMEOUT': '5',
            'LLM_MAX_RETRIES': '1',
            'LLM_WORKER_THREADS': '8',
            
            # é¦–Tokenä¼˜åŒ–
            'LLM_FIRST_TOKEN_PRIORITY': 'true',
            'LLM_STREAM_FIRST_TOKEN': 'true',
            'LLM_PREFILL_OPTIMIZATION': 'true',
            
            # å†…å­˜ä¼˜åŒ–
            'LLM_ENABLE_CACHE': 'true',
            'LLM_CACHE_SIZE_MB': '512',
            'LLM_MEMORY_POOL': 'true',
            
            # ç¡¬ä»¶ä¼˜åŒ–
            'CUDA_VISIBLE_DEVICES': '0',  # ä½¿ç”¨GPU 0
            'TORCH_COMPILE': 'true',
            'FLASH_ATTENTION': 'true',
        }
        
        if optimization_level == 'aggressive':
            env_vars.update({
                'LLM_MAX_NEW_TOKENS': '1',
                'LLM_TEMPERATURE': '0.0',
                'LLM_TOP_K': '1',
                'LLM_DO_SAMPLE': 'false',
                'LLM_TORCH_DTYPE': 'float16',
            })
        
        return env_vars
    
    def save_optimization_config(self, output_path: str, optimization_level: str = 'aggressive'):
        """ä¿å­˜ä¼˜åŒ–é…ç½®åˆ°æ–‡ä»¶"""
        config = self.generate_llm_config(optimization_level)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        print(f"âœ… LLMä¼˜åŒ–é…ç½®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"ğŸ¯ ä¼˜åŒ–çº§åˆ«: {optimization_level}")
        print(f"ğŸ“ˆ é¢„æœŸé¦–Tokenå»¶è¿Ÿå‡å°‘: 150-200ms")

def main():
    """ä¸»å‡½æ•°"""
    optimizer = LLMFirstTokenOptimizer()
    
    # ç”Ÿæˆä¸åŒçº§åˆ«çš„ä¼˜åŒ–é…ç½®
    levels = ['basic_optimization', 'aggressive_optimization', 'streaming_optimization']
    
    for level in levels:
        output_file = f"/root/xiaozhi-server/config/llm_{level.replace('_optimization', '')}_config.yaml"
        optimizer.save_optimization_config(output_file, level)
        
        # æ‰“å°ç¯å¢ƒå˜é‡
        env_vars = optimizer.generate_environment_variables(level)
        print(f"\nğŸ“‹ {level} ç¯å¢ƒå˜é‡:")
        for key, value in env_vars.items():
            print(f"export {key}={value}")
        print("-" * 50)

if __name__ == "__main__":
    main()